<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>ani.dev</title>
  
  
  <link href="/atom.xml" rel="self"/>
  
  <link href="https://ani.dev/"/>
  <updated>2020-03-26T01:39:22.300Z</updated>
  <id>https://ani.dev/</id>
  
  <author>
    <name>Ani Channarasappa</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>Using Terraform, AWS Lambda, and Locust to design and deploy a serverless web scraper system - Part 2/3</title>
    <link href="https://ani.dev/2020/01/26/serverless-apartment-listing-scraper-pt-2/"/>
    <id>https://ani.dev/2020/01/26/serverless-apartment-listing-scraper-pt-2/</id>
    <published>2020-01-26T13:06:28.000Z</published>
    <updated>2020-03-26T01:39:22.300Z</updated>
    
    <content type="html"><![CDATA[<p>This is part two of a three part series in which we’ll seek to understand:</p><p><strong>What areas in New York are most popular, have the best public transit connectivity, and offer the best amenities for their asking price?</strong></p><p>If you haven’t already, check out part one <a href="https://dev.to/achannarasappa/serverless-apartment-web-scraper-with-nodejs-aws-lambda-and-locust-ngk" target="_blank" rel="noopener">here</a> to get caught up.</p><h2 id="Looking-ahead"><a href="#Looking-ahead" class="headerlink" title="Looking ahead"></a>Looking ahead</h2><p>In this article we’ll cover the following:</p><ul><li>Using Terraform to provision the infrastructure for a serverless web crawler</li><li>Setup a recursive serverless function</li><li>Connecting to datastores and external systems</li><li>Schedule a daily run for the crawl job</li><li>Deploying the system to AWS</li></ul><h2 id="Recap"><a href="#Recap" class="headerlink" title="Recap"></a>Recap</h2><p>Thus far, we’ve put together and tested locally a configuration file that defines how the scraper will extract apartment listings from Craigslist. That configuration should look something like this:</p><figure class="highlight js"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// ./src/job.js</span></span><br><span class="line"><span class="keyword">const</span> &#123; Client &#125; = <span class="built_in">require</span>(<span class="string">'pg'</span>)</span><br><span class="line"><span class="keyword">const</span> moment = <span class="built_in">require</span>(<span class="string">'moment'</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">// non-configuration truncated for brevity</span></span><br><span class="line"><span class="comment">// see here for full file: https://github.com/achannarasappa/locust-examples/blob/master/apartment-listings/src/job.js</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">module</span>.exports = &#123;</span><br><span class="line">  extract: <span class="keyword">async</span> ($, page) =&gt; transformListing(&#123;</span><br><span class="line">    <span class="string">'title'</span>: <span class="keyword">await</span> $(<span class="string">'.postingtitletext #titletextonly'</span>),</span><br><span class="line">    <span class="string">'price'</span>: <span class="keyword">await</span> $(<span class="string">'.postingtitletext .price'</span>),</span><br><span class="line">    <span class="string">'housing'</span>: <span class="keyword">await</span> $(<span class="string">'.postingtitletext .housing'</span>),</span><br><span class="line">    <span class="string">'location'</span>: <span class="keyword">await</span> $(<span class="string">'.postingtitletext small'</span>),</span><br><span class="line">    <span class="string">'datetime'</span>: <span class="keyword">await</span> page.$<span class="built_in">eval</span>(<span class="string">'.postinginfo time'</span>, (el) =&gt; el.getAttribute(<span class="string">'datetime'</span>)).catch(<span class="function"><span class="params">()</span> =&gt;</span> <span class="literal">null</span>),</span><br><span class="line">    <span class="string">'images'</span>: <span class="keyword">await</span> page.$$<span class="built_in">eval</span>(<span class="string">'#thumbs .thumb'</span>, (elements) =&gt; elements.map(<span class="function">(<span class="params">el</span>) =&gt;</span> el.getAttribute(<span class="string">'href'</span>))).catch(<span class="function"><span class="params">()</span> =&gt;</span> <span class="literal">null</span>),</span><br><span class="line">    <span class="string">'attributes'</span>: <span class="keyword">await</span> page.$$<span class="built_in">eval</span>(<span class="string">'.mapAndAttrs p.attrgroup:not(:nth-of-type(1)) span'</span>, (elements) =&gt; elements.map(<span class="function">(<span class="params">el</span>) =&gt;</span> el.textContent)).catch(<span class="function"><span class="params">()</span> =&gt;</span> <span class="literal">null</span>),</span><br><span class="line">    <span class="string">'google_maps_link'</span>: <span class="keyword">await</span> page.$<span class="built_in">eval</span>(<span class="string">'.mapaddress a'</span>, (el) =&gt; el.getAttribute(<span class="string">'href'</span>)).catch(<span class="function"><span class="params">()</span> =&gt;</span> <span class="literal">null</span>),</span><br><span class="line">    <span class="string">'description'</span>: <span class="keyword">await</span> $(<span class="string">'#postingbody'</span>),</span><br><span class="line">  &#125;),</span><br><span class="line">  after: <span class="keyword">async</span> (jobResult, snapshot, stop) =&gt; &#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> (isListingUrl(jobResult.response.url)) &#123;</span><br><span class="line"></span><br><span class="line">      <span class="keyword">await</span> saveListing(jobResult.data)</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> (snapshot.queue.done.length &gt;= <span class="number">25</span>)</span><br><span class="line">      <span class="keyword">await</span> stop()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> jobResult;</span><br><span class="line"></span><br><span class="line">  &#125;,</span><br><span class="line">  start: <span class="function"><span class="params">()</span> =&gt;</span> <span class="literal">null</span>,</span><br><span class="line">  url: <span class="string">'https://newyork.craigslist.org/search/apa'</span>,</span><br><span class="line">  config: &#123;</span><br><span class="line">    name: <span class="string">'apartment-listings'</span>,</span><br><span class="line">    concurrencyLimit: <span class="number">2</span>,</span><br><span class="line">    depthLimit: <span class="number">100</span>,</span><br><span class="line">    delay: <span class="number">3000</span>,</span><br><span class="line">  &#125;,</span><br><span class="line">  filter: <span class="function">(<span class="params">links</span>) =&gt;</span> links.filter(<span class="function"><span class="params">link</span> =&gt;</span> isIndexUrl(link) || isListingUrl(link)),</span><br><span class="line">  connection: &#123;</span><br><span class="line">    redis: &#123;</span><br><span class="line">      port: <span class="number">6379</span>,</span><br><span class="line">      host: <span class="string">'localhost'</span></span><br><span class="line">    &#125;,</span><br><span class="line">    chrome: &#123;</span><br><span class="line">      browserWSEndpoint: <span class="string">`ws://localhost:3000`</span>,</span><br><span class="line">    &#125;,</span><br><span class="line">  &#125;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure><p>The next steps are to design the system, set up the infrastructure, and deploy the code.</p><h2 id="System-Design"><a href="#System-Design" class="headerlink" title="System Design"></a>System Design</h2><p>Let’s define some non-functional requirements and considerations to guide the design:</p><ul><li>No pre-existing infrastructure or systems - a greenfield build</li><li>Listings change frequently so the crawl should be run on a regular interval</li><li>Locust requires a Redis and Chrome instance for it’s queue and HTTP requests respectively</li><li>Network access<ul><li>Serverless run context will need network access to the data store for listings</li><li>Serverless run context will need network access to the Redis and Chrome instances for Locust</li><li>Chrome will need access to the internet to execute HTTP requests</li></ul></li><li>A database schema will need to be defined for the data store before it is usable</li></ul><p>With these in mind, the system diagram would look like this:</p><p><img src="https://thepracticaldev.s3.amazonaws.com/i/r09c4ezx7prgx2jubqio.png" alt="system"></p><p>Note: the database will be in the public subnet to simplify initial setup</p><h2 id="Infrastructure-setup"><a href="#Infrastructure-setup" class="headerlink" title="Infrastructure setup"></a>Infrastructure setup</h2><p>To setup and manage infrastructure, we’ll use <a href="https://www.terraform.io/" target="_blank" rel="noopener">Terraform</a> to define our infrastructure as configuration. The some of the Terraform resources needed for this setup are low level and not part of the core problem so we’ll pull in a few Terraform modules that provide higher order abstractions for these common resource collections. These are:</p><ul><li>AWS VPC - <a href="https://github.com/terraform-aws-modules/terraform-aws-vpc" target="_blank" rel="noopener">terraform-aws-modules/vpc/aws</a></li><li>AWS RDS - <a href="https://github.com/terraform-aws-modules/terraform-aws-rds" target="_blank" rel="noopener">terraform-aws-modules/rds/aws</a></li><li>Locust internal resources - <a href="https://github.com/achannarasappa/locust-aws-terraform" target="_blank" rel="noopener">github.com/achannarasappa/locust-aws-terraform</a></li></ul><h3 id="Compute-AWS-Lambda"><a href="#Compute-AWS-Lambda" class="headerlink" title="Compute (AWS Lambda)"></a>Compute (AWS Lambda)</h3><p><img src="https://thepracticaldev.s3.amazonaws.com/i/iht1ebhxeq3w47mq9ntf.png" alt="compute"></p><p>First we’ll start by setting up the Locust job in an AWS Lambda function:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"># .&#x2F;infra&#x2F;main.tf</span><br><span class="line"></span><br><span class="line">provider &quot;aws&quot; &#123;</span><br><span class="line">  profile &#x3D; &quot;default&quot;</span><br><span class="line">  region  &#x3D; &quot;us-east-1&quot;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">resource &quot;aws_lambda_function&quot; &quot;apartment_listings_crawler&quot; &#123;</span><br><span class="line">  function_name    &#x3D; &quot;apartment-listings&quot;</span><br><span class="line">  filename         &#x3D; &quot;.&#x2F;src.zip&quot;</span><br><span class="line">  source_code_hash &#x3D; filebase64sha256(&quot;.&#x2F;src.zip&quot;)</span><br><span class="line"></span><br><span class="line">  handler &#x3D; &quot;src&#x2F;handler.start&quot;</span><br><span class="line">  runtime &#x3D; &quot;nodejs10.x&quot;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>Note here that a handler of <code>src/handler.start</code> is referenced along with a file bundle <code>./src.zip</code>. <code>src/handler.start</code> is the AWS Lambda function handler that is called when the function is triggered. Since with each Locust job run, the next job’s data is pulled from Redis queue, no arguments are needed from the handler and the handler ends up being fairly straightforward:</p><figure class="highlight js"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// ./src/handler.js</span></span><br><span class="line"><span class="keyword">const</span> &#123; execute &#125; = <span class="built_in">require</span>(<span class="string">'@achannarasappa/locust'</span>);</span><br><span class="line"><span class="keyword">const</span> job = <span class="built_in">require</span>(<span class="string">'./job.js'</span>)</span><br><span class="line"></span><br><span class="line"><span class="built_in">module</span>.exports.start = <span class="function"><span class="params">()</span> =&gt;</span> execute(job);</span><br></pre></td></tr></table></figure><p>Next, the source along with dependencies will need to be <a href="https://github.com/achannarasappa/locust-examples/blob/master/apartment-listings/package.json#L7" target="_blank" rel="noopener">bundled into <code>./src.zip</code></a>:</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">npm install &amp;&amp; zip -r ./infra/src.zip ./src package*.json node_modules</span><br></pre></td></tr></table></figure><p>Since <code>source_code_hash</code> has been set to <code>filebase64sha256</code> of the zip file, a rebundle will result in a diff in Terraform and the new file bundle will be pushed up.</p><p>From this point, the lambda can be provisioned to AWS with <code>terraform apply</code> but it won’t be all that useful since it still lacks connection information and network access to other resources in addition to basic permissions to run. We will come back to this Terraform block later to add those pieces once they’ve been setup elsewhere.</p><h3 id="Networking-VPC"><a href="#Networking-VPC" class="headerlink" title="Networking (VPC)"></a>Networking (VPC)</h3><p>In order to provision many of the resources needed for this system, a VPC is required. The <a href="https://github.com/terraform-aws-modules/terraform-aws-vpc" target="_blank" rel="noopener">terraform-aws-modules/vpc/aws</a> module can be used to setup a VPC along with some common resources associated with networking:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"># .&#x2F;infra&#x2F;main.tf</span><br><span class="line"></span><br><span class="line">module &quot;vpc&quot; &#123;</span><br><span class="line">  source &#x3D; &quot;terraform-aws-modules&#x2F;vpc&#x2F;aws&quot;</span><br><span class="line"></span><br><span class="line">  name &#x3D; &quot;apartment-listings&quot;</span><br><span class="line"></span><br><span class="line">  cidr &#x3D; &quot;10.0.0.0&#x2F;16&quot;</span><br><span class="line"></span><br><span class="line">  azs             &#x3D; [&quot;us-east-1c&quot;, &quot;us-east-1d&quot;]</span><br><span class="line">  private_subnets &#x3D; [&quot;10.0.1.0&#x2F;24&quot;, &quot;10.0.2.0&#x2F;24&quot;]</span><br><span class="line">  public_subnets  &#x3D; [&quot;10.0.101.0&#x2F;24&quot;, &quot;10.0.102.0&#x2F;24&quot;]</span><br><span class="line"></span><br><span class="line">  # enable public access to database for initial setup</span><br><span class="line">  create_database_subnet_group           &#x3D; true</span><br><span class="line">  create_database_subnet_route_table     &#x3D; true</span><br><span class="line">  create_database_internet_gateway_route &#x3D; true</span><br><span class="line">  enable_dns_hostnames                   &#x3D; true</span><br><span class="line">  enable_dns_support                     &#x3D; true</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>With the VPC setup, we can start adding resources to it starting with the database</p><h3 id="Storage-AWS-RDS"><a href="#Storage-AWS-RDS" class="headerlink" title="Storage (AWS RDS)"></a>Storage (AWS RDS)</h3><p><img src="https://thepracticaldev.s3.amazonaws.com/i/lgpmtjfn8id6dvs1v5w1.png" alt="database"></p><p>For the database, we’ll need to provision a Postgres instance to AWS RDS along with set up the schema. The configuration for a minimal database will be as follows:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"># .&#x2F;infra&#x2F;main.tf</span><br><span class="line"></span><br><span class="line">module &quot;db&quot; &#123;</span><br><span class="line">  source  &#x3D; &quot;terraform-aws-modules&#x2F;rds&#x2F;aws&quot;</span><br><span class="line">  version &#x3D; &quot;~&gt; 2.0&quot;</span><br><span class="line"></span><br><span class="line">  identifier &#x3D; &quot;apartment-listings-postgres&quot;</span><br><span class="line"></span><br><span class="line">  engine            &#x3D; &quot;postgres&quot;</span><br><span class="line">  engine_version    &#x3D; &quot;10.10&quot;</span><br><span class="line">  instance_class    &#x3D; &quot;db.t3.micro&quot;</span><br><span class="line">  allocated_storage &#x3D; 5</span><br><span class="line">  storage_encrypted &#x3D; false</span><br><span class="line"></span><br><span class="line">  name     &#x3D; var.postgres_database</span><br><span class="line">  username &#x3D; var.postgres_user</span><br><span class="line">  password &#x3D; var.postgres_password</span><br><span class="line">  port     &#x3D; var.postgres_port</span><br><span class="line"></span><br><span class="line">  publicly_accessible &#x3D; true</span><br><span class="line"></span><br><span class="line">  vpc_security_group_ids &#x3D; []</span><br><span class="line"></span><br><span class="line">  maintenance_window      &#x3D; &quot;Mon:00:00-Mon:03:00&quot;</span><br><span class="line">  backup_window           &#x3D; &quot;03:00-06:00&quot;</span><br><span class="line">  backup_retention_period &#x3D; 0</span><br><span class="line">  family                  &#x3D; &quot;postgres10&quot;</span><br><span class="line">  major_engine_version    &#x3D; &quot;10.10&quot;</span><br><span class="line"></span><br><span class="line">  enabled_cloudwatch_logs_exports &#x3D; [&quot;postgresql&quot;, &quot;upgrade&quot;]</span><br><span class="line"></span><br><span class="line">  subnet_ids          &#x3D; module.vpc.public_subnets</span><br><span class="line">  deletion_protection &#x3D; false</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>Note here that the RDS instance is marked as publicly accessible and part of a public subnet so that we can perform the one time setup of the database schema. There are also no <code>vpc_security_group_ids</code> defined yet which will need to be added later.</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line">resource &quot;aws_security_group&quot; &quot;local-database-access&quot; &#123;</span><br><span class="line">  vpc_id &#x3D; &quot;$&#123;module.vpc.vpc_id&#125;&quot;</span><br><span class="line"></span><br><span class="line">  ingress &#123;</span><br><span class="line">    protocol  &#x3D; &quot;-1&quot;</span><br><span class="line">    self      &#x3D; true</span><br><span class="line">    from_port &#x3D; tonumber(var.postgres_port)</span><br><span class="line">    to_port   &#x3D; tonumber(var.postgres_port)</span><br><span class="line">    cidr_blocks &#x3D; [&quot;$&#123;chomp(data.http.myip.body)&#125;&#x2F;32&quot;]</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  egress &#123;</span><br><span class="line">    from_port   &#x3D; 0</span><br><span class="line">    to_port     &#x3D; 0</span><br><span class="line">    protocol    &#x3D; &quot;-1&quot;</span><br><span class="line">    cidr_blocks &#x3D; [&quot;0.0.0.0&#x2F;0&quot;]</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">data &quot;http&quot; &quot;myip&quot; &#123;</span><br><span class="line">  url &#x3D; &quot;http:&#x2F;&#x2F;ipv4.icanhazip.com&quot;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">resource &quot;null_resource&quot; &quot;db_setup&quot; &#123;</span><br><span class="line">  provisioner &quot;local-exec&quot; &#123;</span><br><span class="line">    command &#x3D; &quot;PGPASSWORD&#x3D;$&#123;var.postgres_password&#125; psql -h $&#123;module.db.this_db_instance_address&#125; -p $&#123;var.postgres_port&#125; -f ..&#x2F;db&#x2F;schema&#x2F;setup.sql $&#123;var.postgres_database&#125; $&#123;var.postgres_user&#125;&quot;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>The <code>aws_security_group_rule</code> will add a firewall rule that allows access from the machine being used to provision this system while the <code>null_resource</code> named <code>db_setup</code> will execute an ad-hoc sql query using <a href="https://www.postgresql.org/download/" target="_blank" rel="noopener"><code>psql</code></a> that will create the table and schema in the database (this will run locally so psql will need to be installed on the local machine). The <code>db</code> resource will also need to be updated with the newly created security group for local access:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">vpc_security_group_ids &#x3D; [&quot;$&#123;aws_security_group.local-database-access&#125;&quot;]</span><br></pre></td></tr></table></figure><p>With the infra defined for the database, the we’ll need <a href="https://github.com/achannarasappa/locust-examples/blob/master/apartment-listings/db/schema/setup.sql" target="_blank" rel="noopener">sql statements</a> that sets up the database:</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> listing.home (</span><br><span class="line">    <span class="keyword">id</span> <span class="built_in">integer</span> <span class="keyword">NOT</span> <span class="literal">NULL</span>,</span><br><span class="line">    title <span class="built_in">character</span> <span class="built_in">varying</span>,</span><br><span class="line">    price <span class="built_in">numeric</span>,</span><br><span class="line">    location <span class="built_in">character</span> <span class="built_in">varying</span>,</span><br><span class="line">    bedroom_count <span class="built_in">numeric</span>,</span><br><span class="line">    <span class="keyword">size</span> <span class="built_in">character</span> <span class="built_in">varying</span>,</span><br><span class="line">    date_posted <span class="built_in">timestamp</span> <span class="keyword">with</span> <span class="built_in">time</span> zone,</span><br><span class="line">    <span class="keyword">attributes</span> jsonb,</span><br><span class="line">    images jsonb,</span><br><span class="line">    description <span class="built_in">character</span> <span class="built_in">varying</span>,</span><br><span class="line">    latitude <span class="built_in">character</span> <span class="built_in">varying</span>,</span><br><span class="line">    longitude <span class="built_in">character</span> <span class="built_in">varying</span></span><br><span class="line">);</span><br></pre></td></tr></table></figure><p>Looking back at the <code>./src/job.js</code> file, the properties here correspond 1:1 with the output of the <a href="https://github.com/achannarasappa/locust-examples/blob/master/apartment-listings/src/job.js#L54" target="_blank" rel="noopener"><code>transformListing</code> function</a>.</p><p>Now all the pieces are in place to provision the database. Also note that there are several variables defined in the preceding terraform blocks that will need to defined in <code>variables.tf</code>:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">variable &quot;postgres_user&quot; &#123;</span><br><span class="line">  default &#x3D; &quot;postgres&quot;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">variable &quot;postgres_password&quot; &#123;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">variable &quot;postgres_database&quot; &#123;</span><br><span class="line">  default &#x3D; &quot;postgres&quot;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">variable &quot;postgres_port&quot; &#123;</span><br><span class="line">  default &#x3D; &quot;5432&quot;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="Scheduling-runs-AWS-Cloudwatch"><a href="#Scheduling-runs-AWS-Cloudwatch" class="headerlink" title="Scheduling runs (AWS Cloudwatch)"></a>Scheduling runs (AWS Cloudwatch)</h3><p><img src="https://thepracticaldev.s3.amazonaws.com/i/kmauinbkfcju9g2mw2jj.png" alt="cron"></p><p>In order to have the crawl execute on an interval, a cron-like solution will be needed that interfaces well with AWS Lambda. One way to achieve that is through a scheduled CloudWatch event:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">resource &quot;aws_cloudwatch_event_rule&quot; &quot;apartment_listings_crawler&quot; &#123;</span><br><span class="line">  name        &#x3D; &quot;apartment_listings_crawler&quot;</span><br><span class="line">  description &#x3D; &quot;Crawls apartment listings on a schedule&quot;</span><br><span class="line"></span><br><span class="line">  schedule_expression &#x3D; &quot;rate(1 day)&quot;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">resource &quot;aws_cloudwatch_event_target&quot; &quot;apartment_listings_crawler&quot; &#123;</span><br><span class="line">  rule &#x3D; &quot;$&#123;aws_cloudwatch_event_rule.apartment_listings_crawler.name&#125;&quot;</span><br><span class="line">  arn  &#x3D; &quot;$&#123;aws_lambda_function.apartment_listings_crawler.arn&#125;&quot;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>This will trigger the Lambda once per day which will start a crawler job that will continue until a stop condition is met spawning additional Lambdas bounded by the <a href="https://github.com/achannarasappa/locust-examples/blob/master/apartment-listings/src/job.js#L104" target="_blank" rel="noopener">parameters in the job definition file</a>.</p><p>An additional resource-based permission is needed to allow CloudWatch events to trigger Lambdas:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">resource &quot;aws_lambda_permission&quot; &quot;apartment_listings_crawler&quot; &#123;</span><br><span class="line">  action        &#x3D; &quot;lambda:InvokeFunction&quot;</span><br><span class="line">  function_name &#x3D; &quot;$&#123;aws_lambda_function.apartment_listings_crawler.function_name&#125;&quot;</span><br><span class="line">  principal     &#x3D; &quot;events.amazonaws.com&quot;</span><br><span class="line">  source_arn    &#x3D; aws_cloudwatch_event_rule.apartment_listings_crawler.arn</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="Locust-internal-resources"><a href="#Locust-internal-resources" class="headerlink" title="Locust internal resources"></a>Locust internal resources</h3><p><img src="https://thepracticaldev.s3.amazonaws.com/i/c7vw0etjv9vtblpmtvma.png" alt="locust"></p><p>The last remaining set of resources to add are the chrome instance which Locust will use to execute HTTP requests in a browser context and the Redis instance which will power Locust’s job queue. These are all defined within the Terraform module <a href="https://github.com/achannarasappa/locust-aws-terraform" target="_blank" rel="noopener"><code>github.com/achannarasappa/locust-aws-terraform</code></a>. Inputs for this module are:</p><ul><li><em>vpc_id</em> - VPC id from <code>apartment-listings</code> VPC defined earlier</li><li><em>private_subnet_ids</em> - list of private subnet ids from <code>apartment-listings</code> VPC defined earlier</li><li><em>public_subnet_ids</em> - list of public subnet ids from <code>apartment-listings</code> VPC defined earlier</li></ul><p>And outputs are:</p><ul><li><em>redis_hostname</em> - hostname of the Redis instance which will need to be passed to the AWS Lambda running Locust</li><li><em>chrome_hostname</em> - hostname of the Chrome instance which will need to be passed to the AWS Lambda running Locust</li><li><em>security_group_id</em> - AWS security group that the Redis and Chrome instances are a part of</li><li><em>iam_role_arn</em> - AWS IAM role with the proper permissions to access Chrome, Redis, and run Locust</li></ul><p>We’ll need to revisit the Lambda configuration to add the hostnames, role ARN, and security group with the outputs from this module in the next section. The security group can also be reused by the <code>db</code> module to allow access from the Lambda to Postgres:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">module &quot;db&quot; &#123;</span><br><span class="line">  ...</span><br><span class="line">  vpc_security_group_ids &#x3D; [&quot;$&#123;module.locust.security_group_id&#125;&quot;]</span><br><span class="line">  ...</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="Tying-everything-together"><a href="#Tying-everything-together" class="headerlink" title="Tying everything together"></a>Tying everything together</h2><p><img src="https://thepracticaldev.s3.amazonaws.com/i/nawzn5btc7flzom3t2yl.png" alt="tying"></p><p>Earlier we set up a placeholder Lambda function that was missing a few key pieces that we now have:</p><ul><li>IAM role</li><li>VPC subnets</li><li>Security groups with dependent resources</li><li>Hostnames for Redis and Chrome plus connection information for Postgres</li></ul><p>Now that other resources have been setup, the <code>aws_lambda_function</code> can be updated with this information:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">resource &quot;aws_lambda_function&quot; &quot;apartment_listings_crawler&quot; &#123;</span><br><span class="line">  ...</span><br><span class="line"></span><br><span class="line">  role &#x3D; &quot;$&#123;module.locust.iam_role_arn&#125;&quot;</span><br><span class="line"></span><br><span class="line">  vpc_config &#123;</span><br><span class="line">    subnet_ids         &#x3D; concat(module.vpc.public_subnets, module.vpc.private_subnets)</span><br><span class="line">    security_group_ids &#x3D; [&quot;$&#123;module.locust.security_group_id&#125;&quot;]</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  environment &#123;</span><br><span class="line">    variables &#x3D; &#123;</span><br><span class="line">      CHROME_HOST       &#x3D; &quot;$&#123;module.locust.chrome_hostname&#125;&quot;</span><br><span class="line">      REDIS_HOST        &#x3D; &quot;$&#123;module.locust.redis_hostname&#125;&quot;</span><br><span class="line">      POSTGRES_HOST     &#x3D; &quot;$&#123;module.db.this_db_instance_address&#125;&quot;</span><br><span class="line">      POSTGRES_USER     &#x3D; &quot;$&#123;var.postgres_user&#125;&quot;</span><br><span class="line">      POSTGRES_PASSWORD &#x3D; &quot;$&#123;var.postgres_password&#125;&quot;</span><br><span class="line">      POSTGRES_DATABASE &#x3D; &quot;$&#123;var.postgres_database&#125;&quot;</span><br><span class="line">      POSTGRES_PORT     &#x3D; &quot;$&#123;var.postgres_port&#125;&quot;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>Connection information for dependencies are passed into the Lambda run context to tell Locust <em>where</em> to connect. The security groups, subnets, and IAM role allow the Lambda to make outbound connections to Postgres, Chrome, and Redis.</p><p>Now that connection information for AWS is being passed into the Locust run context, the various <code>localhost</code> references in <code>./src/job.js</code> can be updated to use those environment variables.</p><ol><li><p>In the connection to Postgres (<code>saveListing</code>s function):</p><figure class="highlight js"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">const</span> client = <span class="keyword">new</span> Client(&#123;</span><br><span class="line">  host: process.env.POSTGRES_HOST || <span class="string">'localhost'</span>,</span><br><span class="line">  database: process.env.POSTGRES_DATABASE || <span class="string">'postgres'</span>,</span><br><span class="line">  user: process.env.POSTGRES_USER || <span class="string">'postgres'</span>,</span><br><span class="line">  password: process.env.POSTGRES_PASSWORD || <span class="string">'postgres'</span>,</span><br><span class="line">  port: process.env.POSTGRES_PORT || <span class="number">5432</span>,</span><br><span class="line">&#125;)</span><br></pre></td></tr></table></figure></li><li><p>In the connection object for Redis and Chrome:</p><figure class="highlight js"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">module</span>.exports = &#123;</span><br><span class="line">  <span class="comment">// ...</span></span><br><span class="line">  connection: &#123;</span><br><span class="line">    redis: &#123;</span><br><span class="line">      port: <span class="number">6379</span>,</span><br><span class="line">      host: process.env.REDIS_HOST || <span class="string">'localhost'</span></span><br><span class="line">    &#125;,</span><br><span class="line">    chrome: &#123;</span><br><span class="line">      browserWSEndpoint: <span class="string">`ws://<span class="subst">$&#123;process.env.CHROME_HOST || <span class="string">'localhost'</span>&#125;</span>:3000`</span>,</span><br><span class="line">    &#125;,</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="comment">// ...</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li></ol><p>With all of the connection details setup, the last step is to replace the dummy <a href="https://locust.dev/docs/api#function-start" target="_blank" rel="noopener"><code>start</code> function</a> with a function that will trigger a new job run. This will allow Locust to recursively trigger itself until a <a href="https://locust.dev/docs/concepts#stop-condition" target="_blank" rel="noopener">stop condition</a> is met. In this case, we need to initiate a new Lambda function:</p><figure class="highlight js"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">const</span> AWS = <span class="built_in">require</span>(<span class="string">'aws-sdk'</span>);</span><br><span class="line"><span class="keyword">const</span> lambda = <span class="keyword">new</span> AWS.Lambda(&#123; <span class="attr">apiVersion</span>: <span class="string">'2015-03-31'</span> &#125;);</span><br><span class="line"></span><br><span class="line"><span class="built_in">module</span>.exports = &#123;</span><br><span class="line">  <span class="comment">// ...</span></span><br><span class="line">  start: <span class="function"><span class="params">()</span> =&gt;</span> lambda.invoke(&#123;</span><br><span class="line">    FunctionName: <span class="string">'apartment-listings'</span>,</span><br><span class="line">    InvocationType: <span class="string">'Event'</span>,</span><br><span class="line">  &#125;).promise()</span><br><span class="line">    .catch(<span class="function">(<span class="params">err</span>) =&gt;</span> <span class="built_in">console</span>.log(err, err.stack)),</span><br><span class="line">  <span class="comment">// ...</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="Deploying-to-AWS"><a href="#Deploying-to-AWS" class="headerlink" title="Deploying to AWS"></a>Deploying to AWS</h2><p>The final setup is to provision the infrastructure and push the bundled source for the crawler. With the <code>source_code_hash = filebase64sha256(&quot;./src.zip&quot;)</code> in resource block for <code>aws_lambda_function</code>, the bundle <code>./src.zip</code> will be pushed along with a <code>terraform apply</code> so no distinct step is needed for that.</p><p>Bundle the source:</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">rm -f ./infra/src.zip &amp;&amp; npm install &amp;&amp; zip -r ./infra/src.zip ./src package*.json node_modules</span><br></pre></td></tr></table></figure><p>Double check thar <code>terraform</code> and <code>psql</code> are installed locally then apply the changes with terraform:</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">cd</span> ./infra &amp;&amp; terraform apply -auto-approve</span><br></pre></td></tr></table></figure><p>The provisioning will take about 10 minutes then the system should be up and running. The CloudWatch will automatically trigger the job once a day so no additional ad-hoc commands are need to run the crawler.</p><p>If you’d like to trigger the crawler immediately, this command can be used:</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">aws lambda invoke \</span><br><span class="line">--invocation-type Event \</span><br><span class="line">--<span class="keyword">function</span>-name apartment_listings_crawler \</span><br><span class="line">--region us-east-1  \</span><br><span class="line">--profile default \</span><br><span class="line">out.txt</span><br></pre></td></tr></table></figure><p>Refer to the Locust operational guide for tips on how to manage Locust and debug issues.</p><h2 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h2><p>Thus far in the series, we’ve learned how to build a serverless crawler with Locust in part 1 including:</p><ul><li>Analyzing how web data is related on a particular website and how this can be used by a crawler to discover page on the fly</li><li>Identifying relevant elements of a web page and how to extract them using Web APIs</li><li>Filtering out noise and optimizing crawler efficiency</li><li>Controlling crawler behaviors and setting stop conditions</li><li>Persisting to a datastore</li><li>Cleaning data before persistence</li></ul><p>In this article, we’ve covered how to deploy the crawler to AWS including:</p><ul><li>Using Terraform to provision the infrastructure for a serverless web crawler</li><li>Setup a recursive serverless function</li><li>Connecting to datastores and external systems</li><li>Schedule a daily run for the crawl job</li><li>Deploying the system to AWS</li></ul><p>In the next article in the series, we’ll take a look at the data that’s been gathered by the crawler to come to a data driven answer to the original question of where are the best areas to live in New York City.</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;This is part two of a three part series in which we’ll seek to understand:&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;What areas in New York are most popular, have t
      
    
    </summary>
    
    
    
  </entry>
  
  <entry>
    <title>Serverless apartment web scraper with NodeJS, AWS Lambda, and Locust - Part 1/3</title>
    <link href="https://ani.dev/2019/12/22/serverless-apartment-listing-scraper-pt-1/"/>
    <id>https://ani.dev/2019/12/22/serverless-apartment-listing-scraper-pt-1/</id>
    <published>2019-12-22T15:32:22.000Z</published>
    <updated>2020-03-26T01:39:22.300Z</updated>
    
    <content type="html"><![CDATA[<p>New York’s apartment rental market is competitive with rentals in desireable neighborhoods being rented quickly. Let’s build a Craigslist apartment listing web scraper to understand the market better and make a data driven decision on where to move.</p><p>Let’s focus on this aspect of the of the apartment rental market:</p><p><strong>What areas in New York are most popular, have the best public transit connectivity, and offer the best amenities for their asking price?</strong></p><p>This will be the first of a three part series:</p><ol><li>Gathering rental market data - Building a web scraper </li><li>Gathering rental market data - Deploying and operating the web scraper</li><li>Deriving rental market insights - Analyzing the data</li></ol><h2 id="Solution-Space"><a href="#Solution-Space" class="headerlink" title="Solution Space"></a>Solution Space</h2><p>While there are a number of different tools that can be used for web data extraction, let’s impose some criteria for this project to help refine solution selection.</p><ol><li>Minimize infrastructure costs (idle + active)</li><li>Horizontally scalability of data extraction</li><li>Maintainability of data extraction logic</li></ol><h3 id="Technologies"><a href="#Technologies" class="headerlink" title="Technologies"></a>Technologies</h3><p>The solution space of web data extraction is quite crowded with a number of open source projects and commercial offerings. In this case we will use:</p><ul><li><strong>AWS RDS</strong> (storage)</li><li><strong>AWS Lambda</strong> (compute)</li><li><strong>NodeJS</strong> (runtime)</li><li><a href="https://locust.dev" target="_blank" rel="noopener"><strong>Locust</strong></a> (scraping framework)</li></ul><p>Disclosure: Locust is developed by me</p><h3 id="Approach"><a href="#Approach" class="headerlink" title="Approach"></a>Approach</h3><p>First, we’ll divide the web scraping problem into a more manageable sub-problems:</p><ol><li>Understand site and page structure<ul><li>How to pages relate to one another?</li><li>Which pages contain relevant information?</li><li>What data attributes are useful for this problem?</li><li>Is any processing needed to clean up or restructure the data?</li></ul></li><li>Configuring the web scraper<ul><li>When should the scraper stop gathering listings?</li><li>How can we gather data quickly while being considerate of site load?</li><li>How should we handle error conditions?</li></ul></li><li>Persisting data<ul><li>How do the entities we store relate to one another?</li><li>How do we structure the data we store?</li><li>Should raw output or cleaned/formatted data be stored?</li></ul></li><li>Deployment and infrastructure on AWS<ul><li>What infrastructure do we need to provision on AWS?</li></ul></li></ol><h3 id="Assumptions"><a href="#Assumptions" class="headerlink" title="Assumptions"></a>Assumptions</h3><p>We’ll also need to validate some assumptions during initial discovery and as we begin capturing data:</p><ol><li>Site and page structure<ol><li>There are only two types of pages - indexes and details</li><li>There is only one page structure for each type of entity with minor variations</li></ol></li><li>Site and user behaviors<ol><li>When listings are removed or retired, the unit is taken by a new tenant</li></ol></li></ol><h2 id="Discovery"><a href="#Discovery" class="headerlink" title="Discovery"></a>Discovery</h2><h3 id="Page-categorization"><a href="#Page-categorization" class="headerlink" title="Page categorization"></a>Page categorization</h3><p>Starting by visiting the <a href="https://newyork.craigslist.org/search/apa?s=120" target="_blank" rel="noopener">CL New York page apartment listing page</a> and exploring, there’s ostensibly only two relevant groupings of pages each with different types of information we need to extract:</p><ol><li><strong>Entity index</strong> - list of multiple entities with some limited detail<br> <img src="https://thepracticaldev.s3.amazonaws.com/i/i3snzq7whmzlgurkngbj.png" alt="entity index"></li><li><strong>Entity detail</strong> - detailed information on a single entity<br> <img src="https://thepracticaldev.s3.amazonaws.com/i/txyfjfz3ro2kuyj2much.png" alt="entity detail"></li></ol><h3 id="Page-relationships"><a href="#Page-relationships" class="headerlink" title="Page relationships"></a>Page relationships</h3><p>Web pages are linked to one another with anchor elements (<code>&lt;a&gt;</code> tags). The <code>href</code> attributes of these elements link to other related pages and  can be used to crawl the entirety of the site. Since we’re only interested in the above two type of entities, the only links we are interested in are those to other entities.</p><p>To get an idea of what links are on an entity index and entity detail page, <code>$$(&#39;a&#39;).map(el =&gt; el.href)</code> can be run in Chrome Developer Tools.</p><p><img src="https://thepracticaldev.s3.amazonaws.com/i/xmfk3s3qw5bpdzqsoxwj.png" alt="links on page"></p><p>Here, there are 350+ links from this page which are mostly not relevant or duplicates. However through examining the results, we find that there are two link patterns that correspond to the two types of entities identified above:</p><ol><li>Entity index - <code>https://newyork.craigslist.org/search/apa?s=&lt;page offset&gt;</code></li><li>Entity detail - <code>https://newyork.craigslist.org/&lt;region&gt;/apa/d/&lt;listing name&gt;/&lt;listing id&gt;.html</code></li></ol><p>The scraper will need to bound it’s crawl of the site to these two types of pages.</p><h3 id="Entity-attributes"><a href="#Entity-attributes" class="headerlink" title="Entity attributes"></a>Entity attributes</h3><p>In the previous step, we’ve already identified links as one of the data attributes that need to be extracted to crawl a site. Since the entity information on an entity index page is rather limited, we’ll focus on extracting entity attributes from the entity detail page.</p><p>Since it’s not yet clear at this stage, what listing elements influence apartment popularity, let’s capture as many attributes as possible and cleave away irrelevant attributes at a later time.</p><p>Below are some attributes and their corresponding locations on the page to capture as a first pass:</p><p><img src="https://thepracticaldev.s3.amazonaws.com/i/v5sk1s5gt807a0f36s31.png" alt="page attributes"></p><ul><li>title</li><li>price</li><li>bedroom_count</li><li>size</li><li>attributes</li><li>latitude</li><li>longitude</li></ul><p>For each of these, we’ll need to find the <a href="https://developer.mozilla.org/en-US/docs/Web/CSS/CSS_Selectors" target="_blank" rel="noopener">CSS selectors</a>. In some cases, (e.g. <code>bedroom_count</code>) we’ll need to capture the an element that contains the data attributes value and use regular expressions later on to process the data and extract the information needed.</p><h3 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h3><p>At this point, we have enough understanding of the site to start writing code / configuration. Before moving on from discovery, let’s summarize what we’ve learned about the site:</p><ul><li>There are two types of pages that have data we’re interested in:<ol><li><strong>Entity index</strong> - list of multiple entities with some limited detail<ul><li><strong>Information to extract</strong>: links to other entity indexes and entity detail pages</li><li><strong>Transforms</strong> - filtering out links to extraneous pages that are not entity indexes or entity detail pages</li><li><strong>Outputs</strong> - list of links to entity index and entity detail pages that should be fed back into the web scraper to scrape next</li></ul></li><li><strong>Entity detail</strong> - detailed information on a single entity<ul><li><strong>Information to extract</strong> - attributes of the single entity</li><li><strong>Transforms</strong> - formatting, cleaning, or restructuring entity attributes</li><li><strong>Outputs</strong> - a single entity to persist to a datastore</li></ul></li></ol></li></ul><h2 id="Execution"><a href="#Execution" class="headerlink" title="Execution"></a>Execution</h2><h3 id="Setup"><a href="#Setup" class="headerlink" title="Setup"></a>Setup</h3><p>Refer to the <a href="https://github.com/achannarasappa/locust-examples/tree/master/apartment-listings#setup" target="_blank" rel="noopener">setup section</a> in the example repo for instructions on how to setup the required tools and dependencies to run the subsequent steps locally.</p><h3 id="Approach-1"><a href="#Approach-1" class="headerlink" title="Approach"></a>Approach</h3><p>The high level process flow will look something like this:<br><img src="https://thepracticaldev.s3.amazonaws.com/i/zbjhxqxkaya9bgjzddyz.png" alt="process flow"></p><p>Locust will handle the labeled scraping and queueing steps with the right job configuration file. The only logic that needs to be developed is the integration with the persistence layer.</p><p>Steps 3, 4, and 5 will loop until a stop condition (step 6) is met at which point the crawl will end.</p><h3 id="Defining-the-job"><a href="#Defining-the-job" class="headerlink" title="Defining the job"></a>Defining the job</h3><p>We’ll start by defining some base properties for the job that will govern how it will operate. We’ll choose some reasonable starting values for these and work to refine them as we learn more about the site behaviors and limitations.</p><ul><li>Entrypoint - As is standard for web crawlers, an entrypoint url defines the first page that is crawled and where links to subsequent pages is extracted. A good starting url will link to other relevant pages and in this case, that would be the first entity index page <code>https://newyork.craigslist.org/search/apa</code>.</li><li>Stop Conditions - When should the job stop? As a starting point, we’ll set a depth limit of 2 indicating that the job shouldn’t crawl pages that are more than two degrees of separation from the entrypoint page.</li><li>Throttling - How should we limit the web crawler so it does not put too great a load on the site? Many servers will enforce rate limitations and ban clients that exceed those limitations. We need to define some starting limitations for the crawler to obey so as to not come up against these limitations. We can start with two concurrent job at any given time and introduce a delay of 3000ms before each job.</li></ul><p>Below is a <a href="https://locust.dev/docs/api#object-jobdefinition" target="_blank" rel="noopener">Locust job definition</a> that captures that above:</p><figure class="highlight js"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// job.js</span></span><br><span class="line"><span class="built_in">module</span>.exports = &#123;</span><br><span class="line">  url: <span class="string">'https://newyork.craigslist.org/search/apa'</span>, <span class="comment">// entrypoint url where the job start</span></span><br><span class="line">  config: &#123;</span><br><span class="line">    name: <span class="string">'apartment-listings'</span>,</span><br><span class="line">    concurrencyLimit: <span class="number">2</span>, <span class="comment">// maximum concurrent number of jobs</span></span><br><span class="line">    depthLimit: <span class="number">2</span>, <span class="comment">// maximum link distance of a page from the entrypoint url to be scraped</span></span><br><span class="line">    delay: <span class="number">3000</span>, <span class="comment">// delay in milliseconds before starting a scrape job</span></span><br><span class="line">  &#125;,</span><br><span class="line">  connection: &#123;</span><br><span class="line">    redis: &#123; <span class="comment">// locust queue connection details</span></span><br><span class="line">      port: <span class="number">6379</span>,</span><br><span class="line">      host: <span class="string">'localhost'</span></span><br><span class="line">    &#125;,</span><br><span class="line">    chrome: &#123; <span class="comment">// locust chrome connection details</span></span><br><span class="line">      browserWSEndpoint: <span class="string">'ws://localhost:3000'</span>,</span><br><span class="line">    &#125;,</span><br><span class="line">  &#125;,</span><br><span class="line">  start: <span class="function"><span class="params">()</span> =&gt;</span> <span class="literal">null</span>,</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure><p>Note: Locust’s CLI tool can be used to interactively generate this file with <a href="https://locust.dev/docs/develop#create-a-job" target="_blank" rel="noopener"><code>locust generate</code></a></p><p>Next, let’s test that this job works with <a href="https://locust.dev/docs/develop#run" target="_blank" rel="noopener"><code>locust run job.js</code></a>:</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line">❯ locust run job.js -l</span><br><span class="line">Running <span class="keyword">in</span> single job mode. Queue related hooks and configuration will be ignored. Check docs <span class="keyword">for</span> more information.</span><br><span class="line">response:</span><br><span class="line">  ok:         <span class="literal">true</span></span><br><span class="line">  status:     200</span><br><span class="line">  statusText: OK</span><br><span class="line">  headers:</span><br><span class="line">    last-modified:             Sat, 30 Nov 2019 17:26:56 GMT</span><br><span class="line">    cache-control:             max-age=900, public</span><br><span class="line">    date:                      Sat, 30 Nov 2019 17:26:55 GMT</span><br><span class="line">    content-encoding:          gzip</span><br><span class="line">    vary:                      Accept-Encoding</span><br><span class="line">    content-length:            36348</span><br><span class="line">    content-type:              text/html; charset=utf-8</span><br><span class="line">    x-frame-options:           SAMEORIGIN                                                           </span><br><span class="line">    server:                    Apache</span><br><span class="line">    expires:                   Sat, 30 Nov 2019 17:41:56 GMT</span><br><span class="line">    <span class="built_in">set</span>-cookie:                cl_b=4|c67de625ad2525f94f6b813ca1498758bbff6f5a|1575135224cQqUI;path=/;domain=.craigslist.org;expires=Fri, 01-Jan-2038 00:00:00 GMT</span><br><span class="line">    strict-transport-security: max-age=86400</span><br><span class="line">  url:        https://newyork.craigslist.org/search/apa</span><br><span class="line">links:</span><br><span class="line">  - https://newyork.craigslist.org/</span><br><span class="line">  - https://newyork.craigslist.org/</span><br><span class="line">  - https://post.craigslist.org/c/nyc</span><br><span class="line">  - https://accounts.craigslist.org/login/home</span><br><span class="line">  - https://newyork.craigslist.org/search/apa<span class="comment">#</span></span><br><span class="line">  - https://newyork.craigslist.org/search/apa<span class="comment">#</span></span><br><span class="line">  ...</span><br></pre></td></tr></table></figure><p>Here again we see the ~350 links. Next let’s strip out links to pages that are not relevant.</p><h3 id="Filtering-links"><a href="#Filtering-links" class="headerlink" title="Filtering links"></a>Filtering links</h3><p>In order to filter the links down to just entity index and detail pages, we can apply a <a href="https://locust.dev/docs/api#function-filter" target="_blank" rel="noopener">filter function</a> with a couple regular expressions. Referring back to the two page patterns identified as relevant earlier, these can be converted into regular expressions to bound the pages the job run on.</p><figure class="highlight js"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// job.js</span></span><br><span class="line"><span class="keyword">const</span> isDetailUrl = <span class="function">(<span class="params">url</span>) =&gt;</span> <span class="regexp">/newyork\.craigslist\.org\/(.*)\/?apa\/d\/(.*)\.html(?&lt;!#)$/</span>.test(url);</span><br><span class="line"><span class="keyword">const</span> isIndexUrl = <span class="function">(<span class="params">url</span>) =&gt;</span> <span class="regexp">/newyork\.craigslist\.org\/search\/apa\?s=([0-9]*)$/</span>.test(url);</span><br><span class="line"></span><br><span class="line"><span class="built_in">module</span>.exports = &#123;</span><br><span class="line">  <span class="comment">// ...</span></span><br><span class="line">  filter: <span class="function">(<span class="params">links</span>) =&gt;</span> links.filter(<span class="function"><span class="params">link</span> =&gt;</span> isIndexUrl(link) || isDetailUrl(link)),</span><br><span class="line">  <span class="comment">// ...</span></span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure><p>Running <code>locust run job.js -l</code> again will yield a much less noisy set of links. We still see duplicates however these will be filtered out internally by Locust.</p><h3 id="Extracting-data"><a href="#Extracting-data" class="headerlink" title="Extracting data"></a>Extracting data</h3><p>Using upon the page elements identified earlier, we can add an <a href="https://locust.dev/docs/api#function-extract" target="_blank" rel="noopener">extract function</a> to define entity attributes to extract from the page for our job. We’ll also need to handle cases when an element at a selector does not exist since we have two page structures that need to be handled.</p><figure class="highlight js"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// job.js</span></span><br><span class="line"><span class="built_in">module</span>.exports = &#123;</span><br><span class="line">  <span class="comment">// ...</span></span><br><span class="line">  extract: <span class="keyword">async</span> ($, page) =&gt; (&#123;</span><br><span class="line">    <span class="string">'title'</span>: <span class="keyword">await</span> $(<span class="string">'.postingtitletext #titletextonly'</span>),</span><br><span class="line">    <span class="string">'price'</span>: <span class="keyword">await</span> $(<span class="string">'.postingtitletext .price'</span>),</span><br><span class="line">    <span class="string">'housing'</span>: <span class="keyword">await</span> $(<span class="string">'.postingtitletext .housing'</span>),</span><br><span class="line">    <span class="string">'location'</span>: <span class="keyword">await</span> $(<span class="string">'.postingtitletext small'</span>),</span><br><span class="line">  &#125;),</span><br><span class="line">  <span class="comment">// ...</span></span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure><p>Here, the <code>$</code> convenience function selects the <a href="https://developer.mozilla.org/en-US/docs/Web/API/Node/textContent" target="_blank" rel="noopener">text content</a> of the first element the CSS selector matches.</p><p>We also want to extract out the listing attributes which correspond to multiple HTML elements with attributes we’re interested in. Locuts’ <code>$</code> is design to only extract a single element from the page so we’ll need to use Puppeteer’s version of <a href="https://developer.mozilla.org/en-US/docs/Web/API/Document/querySelectorAll" target="_blank" rel="noopener">Document.querySelectorAll</a>, <a href="https://pptr.dev/#?product=Puppeteer&version=v1.18.1&show=api-pageevalselector-pagefunction-args" target="_blank" rel="noopener">page.$$eval</a> to extract multiple attributes:</p><figure class="highlight js"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// job.js</span></span><br><span class="line"><span class="built_in">module</span>.exports = &#123;</span><br><span class="line">  ...</span><br><span class="line">  extract: <span class="keyword">async</span> ($, page) =&gt; (&#123;</span><br><span class="line">    ...</span><br><span class="line">    <span class="string">'images'</span>: <span class="keyword">await</span> page.$$<span class="built_in">eval</span>(<span class="string">'#thumbs .thumb'</span>, (elements) =&gt; elements.map(<span class="function">(<span class="params">el</span>) =&gt;</span> el.getAttribute(<span class="string">'href'</span>))).catch(<span class="function"><span class="params">()</span> =&gt;</span> <span class="literal">null</span>),</span><br><span class="line">    ...</span><br><span class="line">  &#125;),</span><br><span class="line">  ...</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure><p>Applying the same approach to the other entity attributes identified earlier, we will end up with an extract function that looks something like <a href="https://github.com/achannarasappa/locust-examples/blob/master/apartment-listings/src/job.js#L72" target="_blank" rel="noopener">this</a>:</p><p>Again running this with Locust CLI returns the unformatted data that we expect:</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">❯ locust run job.js   </span><br><span class="line">Running <span class="keyword">in</span> single job mode. Queue related hooks and configuration will be ignored. Check docs <span class="keyword">for</span> more information.</span><br><span class="line">data: </span><br><span class="line">  title:            Great Location 1 Bd Kent Ave</span><br><span class="line">  price:            <span class="variable">$1995</span></span><br><span class="line">  housing:          / 1br - 550ft2 - </span><br><span class="line">  location:          (Bed Sty/ Clinton Hill)</span><br><span class="line">  datetime:         2019-11-30T09:18:35-0500</span><br><span class="line">  images: </span><br><span class="line">    - https://images.craigslist.org/00n0n_4f3tg9LaeXL_600x450.jpg</span><br><span class="line">    - https://images.craigslist.org/00202_6CW2GEUYqb5_600x450.jpg</span><br><span class="line">    - https://images.craigslist.org/01313_dP3ybMPhO0j_600x450.jpg</span><br><span class="line">    - https://images.craigslist.org/00909_71bNJzxnYCJ_600x450.jpg</span><br><span class="line">    - https://images.craigslist.org/00606_aJQr6Xo6hFU_600x450.jpg</span><br><span class="line">    - https://images.craigslist.org/00C0C_9dQLT85mc4e_600x450.jpg</span><br><span class="line">    - https://images.craigslist.org/00Y0Y_b1LXFSOQtEH_600x450.jpg</span><br><span class="line">  attributes: </span><br><span class="line">    - application fee details: <span class="variable">$20</span> credit check</span><br><span class="line">    - broker fee details: one month</span><br><span class="line">    - cats are OK - purrr</span><br><span class="line">    - apartment</span><br><span class="line">    - laundry <span class="keyword">in</span> bldg</span><br><span class="line">    - listed by: Lawrence Amrhein/Exit All Seasons</span><br><span class="line">  google_maps_link: https://www.google.com/maps/preview/@40.694989,-73.959472,16z</span><br><span class="line">url:      https://newyork.craigslist.org/brk/apa/d/brooklyn-great-location-1-bd-kent-ave/7029456524.html</span><br></pre></td></tr></table></figure><p>Looking at a few of the attributes, all the off the data is present but not in a fully usable state (e.g. housing). Next, we’ll setup some transformations to clean up the data before we persist it.</p><h3 id="Transforming-data"><a href="#Transforming-data" class="headerlink" title="Transforming data"></a>Transforming data</h3><p>Some of the data that the page exposes can be used as is however there some attributes that we want to clean, transform, or split. Below are the attributes that we’ll seek to pull from the raw output:</p><ul><li>price - parse into numerical value with two decimal places</li><li>bedroom count - parse number followed by <code>br</code> from <code>housing</code> field</li><li>size - parse number followed by <code>ft2</code> from <code>housing</code> field</li><li>latitude - parse string from <code>google_maps_link</code></li><li>longitude - parse string from <code>google_maps_link</code></li><li>date_posted - parse ISO 8601 datetime from human readable datetime</li></ul><p>That transform function would look like this:</p><figure class="highlight js"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// job.js</span></span><br><span class="line"><span class="keyword">const</span> moment = <span class="built_in">require</span>(<span class="string">'moment'</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">// ...</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">const</span> transformListing = <span class="function">(<span class="params">listing</span>) =&gt;</span> (&#123;</span><br><span class="line">  title: listing.title,</span><br><span class="line">  price: <span class="built_in">parseInt</span>(((listing.price || <span class="string">''</span>).match(<span class="regexp">/\$([0-9]*)/</span>) || [])[<span class="number">1</span>] || <span class="number">0</span>, <span class="number">10</span>),</span><br><span class="line">  location: matchObjectPropertyRegexOrNull(listing, <span class="string">'location'</span>, /\((.*)\)/),</span><br><span class="line">  bedroom_count: matchObjectPropertyRegexOrNull(listing, <span class="string">'housing'</span>, /([<span class="number">0</span><span class="number">-9</span>]*)br/),</span><br><span class="line">  size: matchObjectPropertyRegexOrNull(listing, <span class="string">'housing'</span>, /([<span class="number">0</span><span class="number">-9</span>]*)ft2/),</span><br><span class="line">  date_posted: listing.datetime ? moment(listing.datetime).format(<span class="string">'YYYY-MM-DD HH:mm:ss'</span>) : <span class="literal">null</span>,</span><br><span class="line">  attributes: listing.attributes || [],</span><br><span class="line">  images: listing.images || [],</span><br><span class="line">  description: listing.description,</span><br><span class="line">  latitude: matchObjectPropertyRegexOrNull(listing, <span class="string">'google_maps_link'</span>, /@([<span class="number">0</span><span class="number">-9.</span>-]*),/),</span><br><span class="line">  longitude: matchObjectPropertyRegexOrNull(listing, <span class="string">'google_maps_link'</span>, /,([<span class="number">0</span><span class="number">-9.</span>-]*),/),</span><br><span class="line">&#125;);</span><br><span class="line"></span><br><span class="line"><span class="keyword">const</span> matchObjectPropertyRegexOrNull = <span class="function">(<span class="params">object, property, regex</span>) =&gt;</span> &#123;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">if</span> (!object[property])</span><br><span class="line">    <span class="keyword">return</span> <span class="literal">null</span>;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">if</span> (!object[property].match(regex))</span><br><span class="line">    <span class="keyword">return</span> <span class="literal">null</span>;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">return</span> object[property].match(regex)[<span class="number">1</span>]</span><br><span class="line"></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="built_in">module</span>.exports = &#123;</span><br><span class="line">  extract: <span class="keyword">async</span> ($, page) =&gt; transformListing(&#123;</span><br><span class="line">    <span class="comment">// ...</span></span><br><span class="line">  &#125;),</span><br><span class="line">  <span class="comment">// ...</span></span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure><p>Layering the transform function into the job definition file and running with the CLI, the output should include the transformed output:</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">❯ locust run ./apartment-listings/src/job.js</span><br><span class="line">Running <span class="keyword">in</span> single job mode. Queue related hooks and configuration will be ignored. Check docs <span class="keyword">for</span> more information.</span><br><span class="line">data: </span><br><span class="line">  title:         Great Location 1 Bd Kent Ave</span><br><span class="line">  price:         1995</span><br><span class="line">  location:      Bed Sty/ Clinton Hill</span><br><span class="line">  bedroom_count: 1</span><br><span class="line">  size:          550</span><br><span class="line">  date_posted:   2019-11-30 09:18:35</span><br><span class="line">  attributes: </span><br><span class="line">    - application fee details: <span class="variable">$20</span> credit check</span><br><span class="line">    - broker fee details: one month</span><br><span class="line">    - cats are OK - purrr</span><br><span class="line">    - apartment</span><br><span class="line">    - laundry <span class="keyword">in</span> bldg</span><br><span class="line">    - listed by: Lawrence Amrhein/Exit All Seasons</span><br><span class="line">  images: </span><br><span class="line">    - https://images.craigslist.org/00n0n_4f3tg9LaeXL_600x450.jpg</span><br><span class="line">    - https://images.craigslist.org/00202_6CW2GEUYqb5_600x450.jpg</span><br><span class="line">    - https://images.craigslist.org/01313_dP3ybMPhO0j_600x450.jpg</span><br><span class="line">    - https://images.craigslist.org/00909_71bNJzxnYCJ_600x450.jpg</span><br><span class="line">    - https://images.craigslist.org/00606_aJQr6Xo6hFU_600x450.jpg</span><br><span class="line">    - https://images.craigslist.org/00C0C_9dQLT85mc4e_600x450.jpg</span><br><span class="line">    - https://images.craigslist.org/00Y0Y_b1LXFSOQtEH_600x450.jpg</span><br><span class="line">  latitude:      40.694989</span><br><span class="line">  longitude:     -73.959472</span><br><span class="line">url:      https://newyork.craigslist.org/brk/apa/d/brooklyn-great-location-1-bd-kent-ave/7029456524.html</span><br></pre></td></tr></table></figure><p>With the right data attributes, the next step is to start persisting the data.</p><h3 id="Persisting-data"><a href="#Persisting-data" class="headerlink" title="Persisting data"></a>Persisting data</h3><p>Since the attributes and structure of listing data is consistent for the most part, a relational database is a suitable storage solution. </p><h4 id="Postgres-Setup"><a href="#Postgres-Setup" class="headerlink" title="Postgres Setup"></a>Postgres Setup</h4><p>Let’s proceed with starting up a local Postgres server:</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker run -it -p 5432:5432 --name listings-pg postgres:10</span><br></pre></td></tr></table></figure><p>Then creating a Postgres Schema and table with schema matching the transformed data structure:</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> <span class="keyword">SCHEMA</span> listing;</span><br><span class="line"></span><br><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> listing.home (</span><br><span class="line">    <span class="keyword">id</span> <span class="built_in">integer</span> <span class="keyword">NOT</span> <span class="literal">NULL</span>,</span><br><span class="line">    title <span class="built_in">character</span> <span class="built_in">varying</span>,</span><br><span class="line">    price <span class="built_in">numeric</span>,</span><br><span class="line">    location <span class="built_in">character</span> <span class="built_in">varying</span>,</span><br><span class="line">    bedroom_count <span class="built_in">numeric</span>,</span><br><span class="line">    <span class="keyword">size</span> <span class="built_in">character</span> <span class="built_in">varying</span>,</span><br><span class="line">    date_posted <span class="built_in">timestamp</span> <span class="keyword">with</span> <span class="built_in">time</span> zone,</span><br><span class="line">    <span class="keyword">attributes</span> jsonb,</span><br><span class="line">    images jsonb,</span><br><span class="line">    description <span class="built_in">character</span> <span class="built_in">varying</span>,</span><br><span class="line">    latitude <span class="built_in">character</span> <span class="built_in">varying</span>,</span><br><span class="line">    longitude <span class="built_in">character</span> <span class="built_in">varying</span></span><br><span class="line">);</span><br></pre></td></tr></table></figure><p>With the Postgres database setup with the proper schema, the next step is to update the job to insert listings.</p><h4 id="Updating-the-job"><a href="#Updating-the-job" class="headerlink" title="Updating the job"></a>Updating the job</h4><p>In order to insert a new listing after each job run, a postgres client will be needed and the popular <a href="https://github.com/brianc/node-postgres" target="_blank" rel="noopener"><code>pg</code> library</a> will work.</p><p>In the job file, a connection will also need to be established for each job run since all jobs run in independent AWS Lambda functions along with a call to execute an <code>INSERT</code> query:</p><figure class="highlight js"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// job.js</span></span><br><span class="line"><span class="keyword">const</span> &#123; Client &#125; = <span class="built_in">require</span>(<span class="string">'pg'</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">// ...</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">const</span> saveListing = <span class="keyword">async</span> (listing) =&gt; &#123;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">const</span> client = <span class="keyword">new</span> Client(&#123;</span><br><span class="line">    host: <span class="string">'localhost'</span>,</span><br><span class="line">    database: <span class="string">'postgres'</span>,</span><br><span class="line">    user: <span class="string">'postgres'</span>,</span><br><span class="line">    password: <span class="string">'postgres'</span>,</span><br><span class="line">    port: <span class="number">5432</span>,</span><br><span class="line">  &#125;)</span><br><span class="line">  <span class="keyword">await</span> client</span><br><span class="line">    .connect();</span><br><span class="line">  <span class="keyword">await</span> client.query(&#123;</span><br><span class="line">    text: [</span><br><span class="line">      <span class="string">'INSERT INTO listing.home'</span>,</span><br><span class="line">      <span class="string">'(title, price, "location", bedroom_count, "size", date_posted, "attributes", images, description, latitude, longitude)'</span>,</span><br><span class="line">      <span class="string">'VALUES('</span>,</span><br><span class="line">      <span class="string">'$1,'</span>,</span><br><span class="line">      <span class="string">'$2,'</span>,</span><br><span class="line">      <span class="string">'$3,'</span>,</span><br><span class="line">      <span class="string">'$4,'</span>,</span><br><span class="line">      <span class="string">'$5,'</span>,</span><br><span class="line">      <span class="string">'$6,'</span>,</span><br><span class="line">      <span class="string">'$7,'</span>,</span><br><span class="line">      <span class="string">'$8,'</span>,</span><br><span class="line">      <span class="string">'$9,'</span>,</span><br><span class="line">      <span class="string">'$10,'</span>,</span><br><span class="line">      <span class="string">'$11'</span>,</span><br><span class="line">      <span class="string">');'</span>,</span><br><span class="line">    ].join(<span class="string">' \n'</span>),</span><br><span class="line">    values: <span class="built_in">Object</span>.values(listing),</span><br><span class="line">  &#125;, () =&gt; &#123;</span><br><span class="line">    client.end()</span><br><span class="line">  &#125;);</span><br><span class="line"></span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure><p>Then, a Locust <a href="https://locust.dev/docs/api#function-start" target="_blank" rel="noopener"><code>after</code> hook</a> will need to be added to the job definition file in which the <code>saveListing</code> function will be called after scraping the site and transforming the output data.</p><p><code>saveListing</code> should also only be called on the entity detail pages and not on the entity index pages so a conditional is in order:</p><figure class="highlight js"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// job.js</span></span><br><span class="line"><span class="built_in">module</span>.exports = &#123;</span><br><span class="line">  <span class="comment">// ...</span></span><br><span class="line">  after: <span class="keyword">async</span> (jobResult, snapshot, stop) =&gt; &#123;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// defined earlier for the filter function</span></span><br><span class="line">    <span class="keyword">if</span> (isListingUrl(jobResult.response.url)) &#123;</span><br><span class="line"></span><br><span class="line">      <span class="keyword">await</span> saveListing(jobResult.data)</span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span>;</span><br><span class="line"></span><br><span class="line">  &#125;,</span><br><span class="line">  <span class="comment">// ...</span></span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure><p>With the integration of the persistence layer, the job definition is for the most part complete. The next step is to do a test run of the job locally before deploying to AWS.</p><p>The complete job definition file can be found in <a href="https://github.com/achannarasappa/locust-examples/blob/master/apartment-listings/src/job.js" target="_blank" rel="noopener">the example repo</a>.</p><h3 id="Putting-it-all-together"><a href="#Putting-it-all-together" class="headerlink" title="Putting it all together"></a>Putting it all together</h3><p>Earlier, <code>locust run</code> was used to scrape a single page to validate that the <code>extract</code> function worked as expected with the queue related features of Locust disabled. Before going through the trouble of setting up infrastructure on AWS and pushing the job up, it is best to run the the job locally with <a href="https://locust.dev/docs/develop#run" target="_blank" rel="noopener"><code>locust start</code></a>. This will run the job very similarly to how it will operate on AWS Lambda (or any cloud provider). This will also run a CLI UI that shows active jobs, their status, and queue information which is useful to tracking job progress and uncovering issues with the job.</p><p>First, ensure that dependent systems are up (postgres, redis, chrome) from <a href="https://github.com/achannarasappa/locust-examples/blob/master/apartment-listings/docker-compose.yml" target="_blank" rel="noopener">this docker-compose.yml</a> file and start them if not with <code>docker-compose up</code></p><p>Next, run the start command with the job file and monitor it’s progress:</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">locust start ./job.js</span><br></pre></td></tr></table></figure><p><img src="https://thepracticaldev.s3.amazonaws.com/i/nroko6ie4gb8kxzomg23.png" alt="monitor run"></p><p>Connecting to the Postgres database and <code>SELECT</code>ing contents of the <code>listing.home</code> table, we can observe new listings being added while the job is running:<br><img src="https://thepracticaldev.s3.amazonaws.com/i/r0kyy9d2srjs4fq2le62.png" alt="postgres"></p><p>This is a good indication that the job is stable and is suitable to push up to AWS.</p><p>Up until this point, the we’ve hardcoded configuration for local runs in the job definition file. Before pushing up to AWS, AWS-specific integrations will need to be added including environment variables and a Locust <a href="https://locust.dev/docs/api#function-start" target="_blank" rel="noopener"><code>start</code> hook</a> to define for Locust how to invoke a new Lambda instance on AWS.</p><h2 id="What’s-next"><a href="#What’s-next" class="headerlink" title="What’s next"></a>What’s next</h2><p>In part two, we’ll deploy the scraper to AWS and begin gathering data.</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;New York’s apartment rental market is competitive with rentals in desireable neighborhoods being rented quickly. Let’s build a Craigslist
      
    
    </summary>
    
    
    
  </entry>
  
</feed>
