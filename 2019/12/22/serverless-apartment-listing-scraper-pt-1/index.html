<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
<meta name="viewport"
      content="width=device-width, initial-scale=1.0, maximum-scale=1.0, minimum-scale=1.0">
<meta http-equiv="X-UA-Compatible" content="ie=edge">

    <meta name="author" content="Ani Channarasappa">





<title>Serverless apartment web scraper with NodeJS, AWS Lambda, and Locust - Part 1/3 | ani.dev</title>



    <link rel="icon" href="/favicon.ico">




    <!-- stylesheets list from _config.yml -->
    
    <link rel="stylesheet" href="/css/style.css">
    



    <!-- scripts list from _config.yml -->
    
    <script src="/js/script.js"></script>
    
    <script src="/js/tocbot.min.js"></script>
    



    
    
        
    


<meta name="generator" content="Hexo 4.2.0"><link rel="alternate" href="/atom.xml" title="ani.dev" type="application/atom+xml">
</head>
<body>
    <div class="wrapper">
        <header>
    <nav class="navbar">
        <div class="container">
            <div class="navbar-header header-logo"><a href="/">ani.dev</a></div>
            <div class="menu navbar-right">
                
                    <a class="menu-item" href="/archives">Posts</a>
                
                    <a class="menu-item" href="/">About</a>
                
                <input id="switch_default" type="checkbox" class="switch_default">
                <label for="switch_default" class="toggleBtn"></label>
            </div>
        </div>
    </nav>

    
    <nav class="navbar-mobile" id="nav-mobile">
        <div class="container">
            <div class="navbar-header">
                <div>
                    <a href="/">ani.dev</a><a id="mobile-toggle-theme">·&nbsp;Light</a>
                </div>
                <div class="menu-toggle" onclick="mobileBtn()">&#9776; Menu</div>
            </div>
            <div class="menu" id="mobile-menu">
                
                    <a class="menu-item" href="/archives">Posts</a>
                
                    <a class="menu-item" href="/">About</a>
                
            </div>
        </div>
    </nav>

</header>
<script>
    var mobileBtn = function f() {
        var toggleMenu = document.getElementsByClassName("menu-toggle")[0];
        var mobileMenu = document.getElementById("mobile-menu");
        if(toggleMenu.classList.contains("active")){
           toggleMenu.classList.remove("active")
            mobileMenu.classList.remove("active")
        }else{
            toggleMenu.classList.add("active")
            mobileMenu.classList.add("active")
        }
    }
</script>
        <div class="main">
            <div class="container">
    
    

    
    <article class="post-wrap">
        <header class="post-header">
            <h1 class="post-title">Serverless apartment web scraper with NodeJS, AWS Lambda, and Locust - Part 1/3</h1>
            
                <div class="post-meta">
                    

                    
                        <span class="post-time">
                        Date: <a href="#">December 22, 2019&nbsp;&nbsp;10:32:22</a>
                        </span>
                    
                    
                </div>
            
        </header>

        <div class="post-content">
            <p>New York’s apartment rental market is competitive with rentals in desireable neighborhoods being rented quickly. Let’s build a Craigslist apartment listing web scraper to understand the market better and make a data driven decision on where to move.</p>
<p>Let’s focus on this aspect of the of the apartment rental market:</p>
<p><strong>What areas in New York are most popular, have the best public transit connectivity, and offer the best amenities for their asking price?</strong></p>
<p>This will be the first of a three part series:</p>
<ol>
<li>Gathering rental market data - Building a web scraper </li>
<li>Gathering rental market data - Deploying and operating the web scraper</li>
<li>Deriving rental market insights - Analyzing the data</li>
</ol>
<h2 id="Solution-Space"><a href="#Solution-Space" class="headerlink" title="Solution Space"></a>Solution Space</h2><p>While there are a number of different tools that can be used for web data extraction, let’s impose some criteria for this project to help refine solution selection.</p>
<ol>
<li>Minimize infrastructure costs (idle + active)</li>
<li>Horizontally scalability of data extraction</li>
<li>Maintainability of data extraction logic</li>
</ol>
<h3 id="Technologies"><a href="#Technologies" class="headerlink" title="Technologies"></a>Technologies</h3><p>The solution space of web data extraction is quite crowded with a number of open source projects and commercial offerings. In this case we will use:</p>
<ul>
<li><strong>AWS RDS</strong> (storage)</li>
<li><strong>AWS Lambda</strong> (compute)</li>
<li><strong>NodeJS</strong> (runtime)</li>
<li><a href="https://locust.dev" target="_blank" rel="noopener"><strong>Locust</strong></a> (scraping framework)</li>
</ul>
<p>Disclosure: Locust is developed by me</p>
<h3 id="Approach"><a href="#Approach" class="headerlink" title="Approach"></a>Approach</h3><p>First, we’ll divide the web scraping problem into a more manageable sub-problems:</p>
<ol>
<li>Understand site and page structure<ul>
<li>How to pages relate to one another?</li>
<li>Which pages contain relevant information?</li>
<li>What data attributes are useful for this problem?</li>
<li>Is any processing needed to clean up or restructure the data?</li>
</ul>
</li>
<li>Configuring the web scraper<ul>
<li>When should the scraper stop gathering listings?</li>
<li>How can we gather data quickly while being considerate of site load?</li>
<li>How should we handle error conditions?</li>
</ul>
</li>
<li>Persisting data<ul>
<li>How do the entities we store relate to one another?</li>
<li>How do we structure the data we store?</li>
<li>Should raw output or cleaned/formatted data be stored?</li>
</ul>
</li>
<li>Deployment and infrastructure on AWS<ul>
<li>What infrastructure do we need to provision on AWS?</li>
</ul>
</li>
</ol>
<h3 id="Assumptions"><a href="#Assumptions" class="headerlink" title="Assumptions"></a>Assumptions</h3><p>We’ll also need to validate some assumptions during initial discovery and as we begin capturing data:</p>
<ol>
<li>Site and page structure<ol>
<li>There are only two types of pages - indexes and details</li>
<li>There is only one page structure for each type of entity with minor variations</li>
</ol>
</li>
<li>Site and user behaviors<ol>
<li>When listings are removed or retired, the unit is taken by a new tenant</li>
</ol>
</li>
</ol>
<h2 id="Discovery"><a href="#Discovery" class="headerlink" title="Discovery"></a>Discovery</h2><h3 id="Page-categorization"><a href="#Page-categorization" class="headerlink" title="Page categorization"></a>Page categorization</h3><p>Starting by visiting the <a href="https://newyork.craigslist.org/search/apa?s=120" target="_blank" rel="noopener">CL New York page apartment listing page</a> and exploring, there’s ostensibly only two relevant groupings of pages each with different types of information we need to extract:</p>
<ol>
<li><strong>Entity index</strong> - list of multiple entities with some limited detail<br> <img src="https://thepracticaldev.s3.amazonaws.com/i/i3snzq7whmzlgurkngbj.png" alt="entity index"></li>
<li><strong>Entity detail</strong> - detailed information on a single entity<br> <img src="https://thepracticaldev.s3.amazonaws.com/i/txyfjfz3ro2kuyj2much.png" alt="entity detail"></li>
</ol>
<h3 id="Page-relationships"><a href="#Page-relationships" class="headerlink" title="Page relationships"></a>Page relationships</h3><p>Web pages are linked to one another with anchor elements (<code>&lt;a&gt;</code> tags). The <code>href</code> attributes of these elements link to other related pages and  can be used to crawl the entirety of the site. Since we’re only interested in the above two type of entities, the only links we are interested in are those to other entities.</p>
<p>To get an idea of what links are on an entity index and entity detail page, <code>$$(&#39;a&#39;).map(el =&gt; el.href)</code> can be run in Chrome Developer Tools.</p>
<p><img src="https://thepracticaldev.s3.amazonaws.com/i/xmfk3s3qw5bpdzqsoxwj.png" alt="links on page"></p>
<p>Here, there are 350+ links from this page which are mostly not relevant or duplicates. However through examining the results, we find that there are two link patterns that correspond to the two types of entities identified above:</p>
<ol>
<li>Entity index - <code>https://newyork.craigslist.org/search/apa?s=&lt;page offset&gt;</code></li>
<li>Entity detail - <code>https://newyork.craigslist.org/&lt;region&gt;/apa/d/&lt;listing name&gt;/&lt;listing id&gt;.html</code></li>
</ol>
<p>The scraper will need to bound it’s crawl of the site to these two types of pages.</p>
<h3 id="Entity-attributes"><a href="#Entity-attributes" class="headerlink" title="Entity attributes"></a>Entity attributes</h3><p>In the previous step, we’ve already identified links as one of the data attributes that need to be extracted to crawl a site. Since the entity information on an entity index page is rather limited, we’ll focus on extracting entity attributes from the entity detail page.</p>
<p>Since it’s not yet clear at this stage, what listing elements influence apartment popularity, let’s capture as many attributes as possible and cleave away irrelevant attributes at a later time.</p>
<p>Below are some attributes and their corresponding locations on the page to capture as a first pass:</p>
<p><img src="https://thepracticaldev.s3.amazonaws.com/i/v5sk1s5gt807a0f36s31.png" alt="page attributes"></p>
<ul>
<li>title</li>
<li>price</li>
<li>bedroom_count</li>
<li>size</li>
<li>attributes</li>
<li>latitude</li>
<li>longitude</li>
</ul>
<p>For each of these, we’ll need to find the <a href="https://developer.mozilla.org/en-US/docs/Web/CSS/CSS_Selectors" target="_blank" rel="noopener">CSS selectors</a>. In some cases, (e.g. <code>bedroom_count</code>) we’ll need to capture the an element that contains the data attributes value and use regular expressions later on to process the data and extract the information needed.</p>
<h3 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h3><p>At this point, we have enough understanding of the site to start writing code / configuration. Before moving on from discovery, let’s summarize what we’ve learned about the site:</p>
<ul>
<li>There are two types of pages that have data we’re interested in:<ol>
<li><strong>Entity index</strong> - list of multiple entities with some limited detail<ul>
<li><strong>Information to extract</strong>: links to other entity indexes and entity detail pages</li>
<li><strong>Transforms</strong> - filtering out links to extraneous pages that are not entity indexes or entity detail pages</li>
<li><strong>Outputs</strong> - list of links to entity index and entity detail pages that should be fed back into the web scraper to scrape next</li>
</ul>
</li>
<li><strong>Entity detail</strong> - detailed information on a single entity<ul>
<li><strong>Information to extract</strong> - attributes of the single entity</li>
<li><strong>Transforms</strong> - formatting, cleaning, or restructuring entity attributes</li>
<li><strong>Outputs</strong> - a single entity to persist to a datastore</li>
</ul>
</li>
</ol>
</li>
</ul>
<h2 id="Execution"><a href="#Execution" class="headerlink" title="Execution"></a>Execution</h2><h3 id="Setup"><a href="#Setup" class="headerlink" title="Setup"></a>Setup</h3><p>Refer to the <a href="https://github.com/achannarasappa/locust-examples/tree/master/apartment-listings#setup" target="_blank" rel="noopener">setup section</a> in the example repo for instructions on how to setup the required tools and dependencies to run the subsequent steps locally.</p>
<h3 id="Approach-1"><a href="#Approach-1" class="headerlink" title="Approach"></a>Approach</h3><p>The high level process flow will look something like this:<br><img src="https://thepracticaldev.s3.amazonaws.com/i/zbjhxqxkaya9bgjzddyz.png" alt="process flow"></p>
<p>Locust will handle the labeled scraping and queueing steps with the right job configuration file. The only logic that needs to be developed is the integration with the persistence layer.</p>
<p>Steps 3, 4, and 5 will loop until a stop condition (step 6) is met at which point the crawl will end.</p>
<h3 id="Defining-the-job"><a href="#Defining-the-job" class="headerlink" title="Defining the job"></a>Defining the job</h3><p>We’ll start by defining some base properties for the job that will govern how it will operate. We’ll choose some reasonable starting values for these and work to refine them as we learn more about the site behaviors and limitations.</p>
<ul>
<li>Entrypoint - As is standard for web crawlers, an entrypoint url defines the first page that is crawled and where links to subsequent pages is extracted. A good starting url will link to other relevant pages and in this case, that would be the first entity index page <code>https://newyork.craigslist.org/search/apa</code>.</li>
<li>Stop Conditions - When should the job stop? As a starting point, we’ll set a depth limit of 2 indicating that the job shouldn’t crawl pages that are more than two degrees of separation from the entrypoint page.</li>
<li>Throttling - How should we limit the web crawler so it does not put too great a load on the site? Many servers will enforce rate limitations and ban clients that exceed those limitations. We need to define some starting limitations for the crawler to obey so as to not come up against these limitations. We can start with two concurrent job at any given time and introduce a delay of 3000ms before each job.</li>
</ul>
<p>Below is a <a href="https://locust.dev/docs/api#object-jobdefinition" target="_blank" rel="noopener">Locust job definition</a> that captures that above:</p>
<figure class="highlight js"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// job.js</span></span><br><span class="line"><span class="built_in">module</span>.exports = &#123;</span><br><span class="line">  url: <span class="string">'https://newyork.craigslist.org/search/apa'</span>, <span class="comment">// entrypoint url where the job start</span></span><br><span class="line">  config: &#123;</span><br><span class="line">    name: <span class="string">'apartment-listings'</span>,</span><br><span class="line">    concurrencyLimit: <span class="number">2</span>, <span class="comment">// maximum concurrent number of jobs</span></span><br><span class="line">    depthLimit: <span class="number">2</span>, <span class="comment">// maximum link distance of a page from the entrypoint url to be scraped</span></span><br><span class="line">    delay: <span class="number">3000</span>, <span class="comment">// delay in milliseconds before starting a scrape job</span></span><br><span class="line">  &#125;,</span><br><span class="line">  connection: &#123;</span><br><span class="line">    redis: &#123; <span class="comment">// locust queue connection details</span></span><br><span class="line">      port: <span class="number">6379</span>,</span><br><span class="line">      host: <span class="string">'localhost'</span></span><br><span class="line">    &#125;,</span><br><span class="line">    chrome: &#123; <span class="comment">// locust chrome connection details</span></span><br><span class="line">      browserWSEndpoint: <span class="string">'ws://localhost:3000'</span>,</span><br><span class="line">    &#125;,</span><br><span class="line">  &#125;,</span><br><span class="line">  start: <span class="function"><span class="params">()</span> =&gt;</span> <span class="literal">null</span>,</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure>

<p>Note: Locust’s CLI tool can be used to interactively generate this file with <a href="https://locust.dev/docs/develop#create-a-job" target="_blank" rel="noopener"><code>locust generate</code></a></p>
<p>Next, let’s test that this job works with <a href="https://locust.dev/docs/develop#run" target="_blank" rel="noopener"><code>locust run job.js</code></a>:</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line">❯ locust run job.js -l</span><br><span class="line">Running <span class="keyword">in</span> single job mode. Queue related hooks and configuration will be ignored. Check docs <span class="keyword">for</span> more information.</span><br><span class="line">response:</span><br><span class="line">  ok:         <span class="literal">true</span></span><br><span class="line">  status:     200</span><br><span class="line">  statusText: OK</span><br><span class="line">  headers:</span><br><span class="line">    last-modified:             Sat, 30 Nov 2019 17:26:56 GMT</span><br><span class="line">    cache-control:             max-age=900, public</span><br><span class="line">    date:                      Sat, 30 Nov 2019 17:26:55 GMT</span><br><span class="line">    content-encoding:          gzip</span><br><span class="line">    vary:                      Accept-Encoding</span><br><span class="line">    content-length:            36348</span><br><span class="line">    content-type:              text/html; charset=utf-8</span><br><span class="line">    x-frame-options:           SAMEORIGIN                                                           </span><br><span class="line">    server:                    Apache</span><br><span class="line">    expires:                   Sat, 30 Nov 2019 17:41:56 GMT</span><br><span class="line">    <span class="built_in">set</span>-cookie:                cl_b=4|c67de625ad2525f94f6b813ca1498758bbff6f5a|1575135224cQqUI;path=/;domain=.craigslist.org;expires=Fri, 01-Jan-2038 00:00:00 GMT</span><br><span class="line">    strict-transport-security: max-age=86400</span><br><span class="line">  url:        https://newyork.craigslist.org/search/apa</span><br><span class="line">links:</span><br><span class="line">  - https://newyork.craigslist.org/</span><br><span class="line">  - https://newyork.craigslist.org/</span><br><span class="line">  - https://post.craigslist.org/c/nyc</span><br><span class="line">  - https://accounts.craigslist.org/login/home</span><br><span class="line">  - https://newyork.craigslist.org/search/apa<span class="comment">#</span></span><br><span class="line">  - https://newyork.craigslist.org/search/apa<span class="comment">#</span></span><br><span class="line">  ...</span><br></pre></td></tr></table></figure>

<p>Here again we see the ~350 links. Next let’s strip out links to pages that are not relevant.</p>
<h3 id="Filtering-links"><a href="#Filtering-links" class="headerlink" title="Filtering links"></a>Filtering links</h3><p>In order to filter the links down to just entity index and detail pages, we can apply a <a href="https://locust.dev/docs/api#function-filter" target="_blank" rel="noopener">filter function</a> with a couple regular expressions. Referring back to the two page patterns identified as relevant earlier, these can be converted into regular expressions to bound the pages the job run on.</p>
<figure class="highlight js"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// job.js</span></span><br><span class="line"><span class="keyword">const</span> isDetailUrl = <span class="function">(<span class="params">url</span>) =&gt;</span> <span class="regexp">/newyork\.craigslist\.org\/(.*)\/?apa\/d\/(.*)\.html(?&lt;!#)$/</span>.test(url);</span><br><span class="line"><span class="keyword">const</span> isIndexUrl = <span class="function">(<span class="params">url</span>) =&gt;</span> <span class="regexp">/newyork\.craigslist\.org\/search\/apa\?s=([0-9]*)$/</span>.test(url);</span><br><span class="line"></span><br><span class="line"><span class="built_in">module</span>.exports = &#123;</span><br><span class="line">  <span class="comment">// ...</span></span><br><span class="line">  filter: <span class="function">(<span class="params">links</span>) =&gt;</span> links.filter(<span class="function"><span class="params">link</span> =&gt;</span> isIndexUrl(link) || isDetailUrl(link)),</span><br><span class="line">  <span class="comment">// ...</span></span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure>

<p>Running <code>locust run job.js -l</code> again will yield a much less noisy set of links. We still see duplicates however these will be filtered out internally by Locust.</p>
<h3 id="Extracting-data"><a href="#Extracting-data" class="headerlink" title="Extracting data"></a>Extracting data</h3><p>Using upon the page elements identified earlier, we can add an <a href="https://locust.dev/docs/api#function-extract" target="_blank" rel="noopener">extract function</a> to define entity attributes to extract from the page for our job. We’ll also need to handle cases when an element at a selector does not exist since we have two page structures that need to be handled.</p>
<figure class="highlight js"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// job.js</span></span><br><span class="line"><span class="built_in">module</span>.exports = &#123;</span><br><span class="line">  <span class="comment">// ...</span></span><br><span class="line">  extract: <span class="keyword">async</span> ($, page) =&gt; (&#123;</span><br><span class="line">    <span class="string">'title'</span>: <span class="keyword">await</span> $(<span class="string">'.postingtitletext #titletextonly'</span>),</span><br><span class="line">    <span class="string">'price'</span>: <span class="keyword">await</span> $(<span class="string">'.postingtitletext .price'</span>),</span><br><span class="line">    <span class="string">'housing'</span>: <span class="keyword">await</span> $(<span class="string">'.postingtitletext .housing'</span>),</span><br><span class="line">    <span class="string">'location'</span>: <span class="keyword">await</span> $(<span class="string">'.postingtitletext small'</span>),</span><br><span class="line">  &#125;),</span><br><span class="line">  <span class="comment">// ...</span></span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure>

<p>Here, the <code>$</code> convenience function selects the <a href="https://developer.mozilla.org/en-US/docs/Web/API/Node/textContent" target="_blank" rel="noopener">text content</a> of the first element the CSS selector matches.</p>
<p>We also want to extract out the listing attributes which correspond to multiple HTML elements with attributes we’re interested in. Locuts’ <code>$</code> is design to only extract a single element from the page so we’ll need to use Puppeteer’s version of <a href="https://developer.mozilla.org/en-US/docs/Web/API/Document/querySelectorAll" target="_blank" rel="noopener">Document.querySelectorAll</a>, <a href="https://pptr.dev/#?product=Puppeteer&version=v1.18.1&show=api-pageevalselector-pagefunction-args" target="_blank" rel="noopener">page.$$eval</a> to extract multiple attributes:</p>
<figure class="highlight js"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// job.js</span></span><br><span class="line"><span class="built_in">module</span>.exports = &#123;</span><br><span class="line">  ...</span><br><span class="line">  extract: <span class="keyword">async</span> ($, page) =&gt; (&#123;</span><br><span class="line">    ...</span><br><span class="line">    <span class="string">'images'</span>: <span class="keyword">await</span> page.$$<span class="built_in">eval</span>(<span class="string">'#thumbs .thumb'</span>, (elements) =&gt; elements.map(<span class="function">(<span class="params">el</span>) =&gt;</span> el.getAttribute(<span class="string">'href'</span>))).catch(<span class="function"><span class="params">()</span> =&gt;</span> <span class="literal">null</span>),</span><br><span class="line">    ...</span><br><span class="line">  &#125;),</span><br><span class="line">  ...</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure>

<p>Applying the same approach to the other entity attributes identified earlier, we will end up with an extract function that looks something like <a href="https://github.com/achannarasappa/locust-examples/blob/master/apartment-listings/src/job.js#L72" target="_blank" rel="noopener">this</a>:</p>
<p>Again running this with Locust CLI returns the unformatted data that we expect:</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">❯ locust run job.js   </span><br><span class="line">Running <span class="keyword">in</span> single job mode. Queue related hooks and configuration will be ignored. Check docs <span class="keyword">for</span> more information.</span><br><span class="line">data: </span><br><span class="line">  title:            Great Location 1 Bd Kent Ave</span><br><span class="line">  price:            <span class="variable">$1995</span></span><br><span class="line">  housing:          / 1br - 550ft2 - </span><br><span class="line">  location:          (Bed Sty/ Clinton Hill)</span><br><span class="line">  datetime:         2019-11-30T09:18:35-0500</span><br><span class="line">  images: </span><br><span class="line">    - https://images.craigslist.org/00n0n_4f3tg9LaeXL_600x450.jpg</span><br><span class="line">    - https://images.craigslist.org/00202_6CW2GEUYqb5_600x450.jpg</span><br><span class="line">    - https://images.craigslist.org/01313_dP3ybMPhO0j_600x450.jpg</span><br><span class="line">    - https://images.craigslist.org/00909_71bNJzxnYCJ_600x450.jpg</span><br><span class="line">    - https://images.craigslist.org/00606_aJQr6Xo6hFU_600x450.jpg</span><br><span class="line">    - https://images.craigslist.org/00C0C_9dQLT85mc4e_600x450.jpg</span><br><span class="line">    - https://images.craigslist.org/00Y0Y_b1LXFSOQtEH_600x450.jpg</span><br><span class="line">  attributes: </span><br><span class="line">    - application fee details: <span class="variable">$20</span> credit check</span><br><span class="line">    - broker fee details: one month</span><br><span class="line">    - cats are OK - purrr</span><br><span class="line">    - apartment</span><br><span class="line">    - laundry <span class="keyword">in</span> bldg</span><br><span class="line">    - listed by: Lawrence Amrhein/Exit All Seasons</span><br><span class="line">  google_maps_link: https://www.google.com/maps/preview/@40.694989,-73.959472,16z</span><br><span class="line">url:      https://newyork.craigslist.org/brk/apa/d/brooklyn-great-location-1-bd-kent-ave/7029456524.html</span><br></pre></td></tr></table></figure>

<p>Looking at a few of the attributes, all the off the data is present but not in a fully usable state (e.g. housing). Next, we’ll setup some transformations to clean up the data before we persist it.</p>
<h3 id="Transforming-data"><a href="#Transforming-data" class="headerlink" title="Transforming data"></a>Transforming data</h3><p>Some of the data that the page exposes can be used as is however there some attributes that we want to clean, transform, or split. Below are the attributes that we’ll seek to pull from the raw output:</p>
<ul>
<li>price - parse into numerical value with two decimal places</li>
<li>bedroom count - parse number followed by <code>br</code> from <code>housing</code> field</li>
<li>size - parse number followed by <code>ft2</code> from <code>housing</code> field</li>
<li>latitude - parse string from <code>google_maps_link</code></li>
<li>longitude - parse string from <code>google_maps_link</code></li>
<li>date_posted - parse ISO 8601 datetime from human readable datetime</li>
</ul>
<p>That transform function would look like this:</p>
<figure class="highlight js"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// job.js</span></span><br><span class="line"><span class="keyword">const</span> moment = <span class="built_in">require</span>(<span class="string">'moment'</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">// ...</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">const</span> transformListing = <span class="function">(<span class="params">listing</span>) =&gt;</span> (&#123;</span><br><span class="line">  title: listing.title,</span><br><span class="line">  price: <span class="built_in">parseInt</span>(((listing.price || <span class="string">''</span>).match(<span class="regexp">/\$([0-9]*)/</span>) || [])[<span class="number">1</span>] || <span class="number">0</span>, <span class="number">10</span>),</span><br><span class="line">  location: matchObjectPropertyRegexOrNull(listing, <span class="string">'location'</span>, /\((.*)\)/),</span><br><span class="line">  bedroom_count: matchObjectPropertyRegexOrNull(listing, <span class="string">'housing'</span>, /([<span class="number">0</span><span class="number">-9</span>]*)br/),</span><br><span class="line">  size: matchObjectPropertyRegexOrNull(listing, <span class="string">'housing'</span>, /([<span class="number">0</span><span class="number">-9</span>]*)ft2/),</span><br><span class="line">  date_posted: listing.datetime ? moment(listing.datetime).format(<span class="string">'YYYY-MM-DD HH:mm:ss'</span>) : <span class="literal">null</span>,</span><br><span class="line">  attributes: listing.attributes || [],</span><br><span class="line">  images: listing.images || [],</span><br><span class="line">  description: listing.description,</span><br><span class="line">  latitude: matchObjectPropertyRegexOrNull(listing, <span class="string">'google_maps_link'</span>, /@([<span class="number">0</span><span class="number">-9.</span>-]*),/),</span><br><span class="line">  longitude: matchObjectPropertyRegexOrNull(listing, <span class="string">'google_maps_link'</span>, /,([<span class="number">0</span><span class="number">-9.</span>-]*),/),</span><br><span class="line">&#125;);</span><br><span class="line"></span><br><span class="line"><span class="keyword">const</span> matchObjectPropertyRegexOrNull = <span class="function">(<span class="params">object, property, regex</span>) =&gt;</span> &#123;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">if</span> (!object[property])</span><br><span class="line">    <span class="keyword">return</span> <span class="literal">null</span>;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">if</span> (!object[property].match(regex))</span><br><span class="line">    <span class="keyword">return</span> <span class="literal">null</span>;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">return</span> object[property].match(regex)[<span class="number">1</span>]</span><br><span class="line"></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="built_in">module</span>.exports = &#123;</span><br><span class="line">  extract: <span class="keyword">async</span> ($, page) =&gt; transformListing(&#123;</span><br><span class="line">    <span class="comment">// ...</span></span><br><span class="line">  &#125;),</span><br><span class="line">  <span class="comment">// ...</span></span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure>

<p>Layering the transform function into the job definition file and running with the CLI, the output should include the transformed output:</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">❯ locust run ./apartment-listings/src/job.js</span><br><span class="line">Running <span class="keyword">in</span> single job mode. Queue related hooks and configuration will be ignored. Check docs <span class="keyword">for</span> more information.</span><br><span class="line">data: </span><br><span class="line">  title:         Great Location 1 Bd Kent Ave</span><br><span class="line">  price:         1995</span><br><span class="line">  location:      Bed Sty/ Clinton Hill</span><br><span class="line">  bedroom_count: 1</span><br><span class="line">  size:          550</span><br><span class="line">  date_posted:   2019-11-30 09:18:35</span><br><span class="line">  attributes: </span><br><span class="line">    - application fee details: <span class="variable">$20</span> credit check</span><br><span class="line">    - broker fee details: one month</span><br><span class="line">    - cats are OK - purrr</span><br><span class="line">    - apartment</span><br><span class="line">    - laundry <span class="keyword">in</span> bldg</span><br><span class="line">    - listed by: Lawrence Amrhein/Exit All Seasons</span><br><span class="line">  images: </span><br><span class="line">    - https://images.craigslist.org/00n0n_4f3tg9LaeXL_600x450.jpg</span><br><span class="line">    - https://images.craigslist.org/00202_6CW2GEUYqb5_600x450.jpg</span><br><span class="line">    - https://images.craigslist.org/01313_dP3ybMPhO0j_600x450.jpg</span><br><span class="line">    - https://images.craigslist.org/00909_71bNJzxnYCJ_600x450.jpg</span><br><span class="line">    - https://images.craigslist.org/00606_aJQr6Xo6hFU_600x450.jpg</span><br><span class="line">    - https://images.craigslist.org/00C0C_9dQLT85mc4e_600x450.jpg</span><br><span class="line">    - https://images.craigslist.org/00Y0Y_b1LXFSOQtEH_600x450.jpg</span><br><span class="line">  latitude:      40.694989</span><br><span class="line">  longitude:     -73.959472</span><br><span class="line">url:      https://newyork.craigslist.org/brk/apa/d/brooklyn-great-location-1-bd-kent-ave/7029456524.html</span><br></pre></td></tr></table></figure>

<p>With the right data attributes, the next step is to start persisting the data.</p>
<h3 id="Persisting-data"><a href="#Persisting-data" class="headerlink" title="Persisting data"></a>Persisting data</h3><p>Since the attributes and structure of listing data is consistent for the most part, a relational database is a suitable storage solution. </p>
<h4 id="Postgres-Setup"><a href="#Postgres-Setup" class="headerlink" title="Postgres Setup"></a>Postgres Setup</h4><p>Let’s proceed with starting up a local Postgres server:</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker run -it -p 5432:5432 --name listings-pg postgres:10</span><br></pre></td></tr></table></figure>

<p>Then creating a Postgres Schema and table with schema matching the transformed data structure:</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> <span class="keyword">SCHEMA</span> listing;</span><br><span class="line"></span><br><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> listing.home (</span><br><span class="line">    <span class="keyword">id</span> <span class="built_in">integer</span> <span class="keyword">NOT</span> <span class="literal">NULL</span>,</span><br><span class="line">    title <span class="built_in">character</span> <span class="built_in">varying</span>,</span><br><span class="line">    price <span class="built_in">numeric</span>,</span><br><span class="line">    location <span class="built_in">character</span> <span class="built_in">varying</span>,</span><br><span class="line">    bedroom_count <span class="built_in">numeric</span>,</span><br><span class="line">    <span class="keyword">size</span> <span class="built_in">character</span> <span class="built_in">varying</span>,</span><br><span class="line">    date_posted <span class="built_in">timestamp</span> <span class="keyword">with</span> <span class="built_in">time</span> zone,</span><br><span class="line">    <span class="keyword">attributes</span> jsonb,</span><br><span class="line">    images jsonb,</span><br><span class="line">    description <span class="built_in">character</span> <span class="built_in">varying</span>,</span><br><span class="line">    latitude <span class="built_in">character</span> <span class="built_in">varying</span>,</span><br><span class="line">    longitude <span class="built_in">character</span> <span class="built_in">varying</span></span><br><span class="line">);</span><br></pre></td></tr></table></figure>

<p>With the Postgres database setup with the proper schema, the next step is to update the job to insert listings.</p>
<h4 id="Updating-the-job"><a href="#Updating-the-job" class="headerlink" title="Updating the job"></a>Updating the job</h4><p>In order to insert a new listing after each job run, a postgres client will be needed and the popular <a href="https://github.com/brianc/node-postgres" target="_blank" rel="noopener"><code>pg</code> library</a> will work.</p>
<p>In the job file, a connection will also need to be established for each job run since all jobs run in independent AWS Lambda functions along with a call to execute an <code>INSERT</code> query:</p>
<figure class="highlight js"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// job.js</span></span><br><span class="line"><span class="keyword">const</span> &#123; Client &#125; = <span class="built_in">require</span>(<span class="string">'pg'</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">// ...</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">const</span> saveListing = <span class="keyword">async</span> (listing) =&gt; &#123;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">const</span> client = <span class="keyword">new</span> Client(&#123;</span><br><span class="line">    host: <span class="string">'localhost'</span>,</span><br><span class="line">    database: <span class="string">'postgres'</span>,</span><br><span class="line">    user: <span class="string">'postgres'</span>,</span><br><span class="line">    password: <span class="string">'postgres'</span>,</span><br><span class="line">    port: <span class="number">5432</span>,</span><br><span class="line">  &#125;)</span><br><span class="line">  <span class="keyword">await</span> client</span><br><span class="line">    .connect();</span><br><span class="line">  <span class="keyword">await</span> client.query(&#123;</span><br><span class="line">    text: [</span><br><span class="line">      <span class="string">'INSERT INTO listing.home'</span>,</span><br><span class="line">      <span class="string">'(title, price, "location", bedroom_count, "size", date_posted, "attributes", images, description, latitude, longitude)'</span>,</span><br><span class="line">      <span class="string">'VALUES('</span>,</span><br><span class="line">      <span class="string">'$1,'</span>,</span><br><span class="line">      <span class="string">'$2,'</span>,</span><br><span class="line">      <span class="string">'$3,'</span>,</span><br><span class="line">      <span class="string">'$4,'</span>,</span><br><span class="line">      <span class="string">'$5,'</span>,</span><br><span class="line">      <span class="string">'$6,'</span>,</span><br><span class="line">      <span class="string">'$7,'</span>,</span><br><span class="line">      <span class="string">'$8,'</span>,</span><br><span class="line">      <span class="string">'$9,'</span>,</span><br><span class="line">      <span class="string">'$10,'</span>,</span><br><span class="line">      <span class="string">'$11'</span>,</span><br><span class="line">      <span class="string">');'</span>,</span><br><span class="line">    ].join(<span class="string">' \n'</span>),</span><br><span class="line">    values: <span class="built_in">Object</span>.values(listing),</span><br><span class="line">  &#125;, () =&gt; &#123;</span><br><span class="line">    client.end()</span><br><span class="line">  &#125;);</span><br><span class="line"></span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure>

<p>Then, a Locust <a href="https://locust.dev/docs/api#function-start" target="_blank" rel="noopener"><code>after</code> hook</a> will need to be added to the job definition file in which the <code>saveListing</code> function will be called after scraping the site and transforming the output data.</p>
<p><code>saveListing</code> should also only be called on the entity detail pages and not on the entity index pages so a conditional is in order:</p>
<figure class="highlight js"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// job.js</span></span><br><span class="line"><span class="built_in">module</span>.exports = &#123;</span><br><span class="line">  <span class="comment">// ...</span></span><br><span class="line">  after: <span class="keyword">async</span> (jobResult, snapshot, stop) =&gt; &#123;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// defined earlier for the filter function</span></span><br><span class="line">    <span class="keyword">if</span> (isListingUrl(jobResult.response.url)) &#123;</span><br><span class="line"></span><br><span class="line">      <span class="keyword">await</span> saveListing(jobResult.data)</span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span>;</span><br><span class="line"></span><br><span class="line">  &#125;,</span><br><span class="line">  <span class="comment">// ...</span></span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure>

<p>With the integration of the persistence layer, the job definition is for the most part complete. The next step is to do a test run of the job locally before deploying to AWS.</p>
<p>The complete job definition file can be found in <a href="https://github.com/achannarasappa/locust-examples/blob/master/apartment-listings/src/job.js" target="_blank" rel="noopener">the example repo</a>.</p>
<h3 id="Putting-it-all-together"><a href="#Putting-it-all-together" class="headerlink" title="Putting it all together"></a>Putting it all together</h3><p>Earlier, <code>locust run</code> was used to scrape a single page to validate that the <code>extract</code> function worked as expected with the queue related features of Locust disabled. Before going through the trouble of setting up infrastructure on AWS and pushing the job up, it is best to run the the job locally with <a href="https://locust.dev/docs/develop#run" target="_blank" rel="noopener"><code>locust start</code></a>. This will run the job very similarly to how it will operate on AWS Lambda (or any cloud provider). This will also run a CLI UI that shows active jobs, their status, and queue information which is useful to tracking job progress and uncovering issues with the job.</p>
<p>First, ensure that dependent systems are up (postgres, redis, chrome) from <a href="https://github.com/achannarasappa/locust-examples/blob/master/apartment-listings/docker-compose.yml" target="_blank" rel="noopener">this docker-compose.yml</a> file and start them if not with <code>docker-compose up</code></p>
<p>Next, run the start command with the job file and monitor it’s progress:</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">locust start ./job.js</span><br></pre></td></tr></table></figure>
<p><img src="https://thepracticaldev.s3.amazonaws.com/i/nroko6ie4gb8kxzomg23.png" alt="monitor run"></p>
<p>Connecting to the Postgres database and <code>SELECT</code>ing contents of the <code>listing.home</code> table, we can observe new listings being added while the job is running:<br><img src="https://thepracticaldev.s3.amazonaws.com/i/r0kyy9d2srjs4fq2le62.png" alt="postgres"></p>
<p>This is a good indication that the job is stable and is suitable to push up to AWS.</p>
<p>Up until this point, the we’ve hardcoded configuration for local runs in the job definition file. Before pushing up to AWS, AWS-specific integrations will need to be added including environment variables and a Locust <a href="https://locust.dev/docs/api#function-start" target="_blank" rel="noopener"><code>start</code> hook</a> to define for Locust how to invoke a new Lambda instance on AWS.</p>
<h2 id="What’s-next"><a href="#What’s-next" class="headerlink" title="What’s next"></a>What’s next</h2><p>In part two, we’ll deploy the scraper to AWS and begin gathering data.</p>

        </div>

        
        <section class="post-tags">
            <div>
                <span>Tag(s):</span>
                <span class="tag">
                    
                </span>
            </div>
            <div>
                <a href="javascript:window.history.back();">back</a>
                <span>· </span>
                <a href="/">home</a>
            </div>
        </section>
        <section class="post-nav">
            
                <a class="prev" rel="prev" href="/2020/01/26/serverless-apartment-listing-scraper-pt-2/">Using Terraform, AWS Lambda, and Locust to design and deploy a serverless web scraper system - Part 2/3</a>
            
            
        </section>


    </article>
</div>

        </div>
        <footer id="footer" class="footer">
    <div class="copyright">
        <span>© Ani Channarasappa </span>
    </div>
</footer>
    </div>
</body>
</html>
